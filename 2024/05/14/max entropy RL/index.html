<!DOCTYPE html>
<html lang=en>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="renderer" content="webkit">
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="format-detection" content="telephone=no,email=no,adress=no">
  <!-- Color theme for statusbar -->
  <meta name="theme-color" content="#000000" />
  <!-- 强制页面在当前窗口以独立页面显示,防止别人在框架里调用页面 -->
  <meta http-equiv="window-target" content="_top" />
  
  
  <title>最大熵强化学习——从概率图模型到SAC | emoairx</title>
  <meta name="description" content="CS285 Lec14: Control as Inference Problem - 知乎 (zhihu.com) 这个讲得是真的好  我们最初有的设定是  p(O_t\mid s_t,a_t)\propto \exp (r(s_t,a_t))对于给定的一张概率图  \begin{align*} \beta_t(s_t,a_t)&amp;&#x3D;p(O_{t:T}\mid s_t,a_t)\\ &amp;&#x3D;\int">
<meta property="og:type" content="article">
<meta property="og:title" content="最大熵强化学习——从概率图模型到SAC">
<meta property="og:url" content="http://emoairx.github.io/blog/2024/05/14/max%20entropy%20RL/index.html">
<meta property="og:site_name" content="Emoairx">
<meta property="og:description" content="CS285 Lec14: Control as Inference Problem - 知乎 (zhihu.com) 这个讲得是真的好  我们最初有的设定是  p(O_t\mid s_t,a_t)\propto \exp (r(s_t,a_t))对于给定的一张概率图  \begin{align*} \beta_t(s_t,a_t)&amp;&#x3D;p(O_{t:T}\mid s_t,a_t)\\ &amp;&#x3D;\int">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://cdn.luogu.com.cn/upload/image_hosting/zs0zum8g.png">
<meta property="og:image" content="https://cdn.luogu.com.cn/upload/image_hosting/ygmz2jdc.png">
<meta property="article:published_time" content="2024-05-14T10:43:23.367Z">
<meta property="article:modified_time" content="2024-06-03T12:32:22.468Z">
<meta property="article:author" content="emoairx">
<meta property="article:tag" content="笔记">
<meta property="article:tag" content="强化学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.luogu.com.cn/upload/image_hosting/zs0zum8g.png">
  <!-- Canonical links -->
  <link rel="canonical" href="http://emoairx.github.io/blog/2024/05/14/max%20entropy%20RL/index.html">
  
    <link rel="alternate" href="/atom.xml" title="Emoairx" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png" type="image/x-icon">
  
  
<link rel="stylesheet" href="/blog/css/style.css">

  
  
  
  
<meta name="generator" content="Hexo 5.4.0"></head>


<body class="main-center" itemscope itemtype="http://schema.org/WebPage">
  <header class="header" itemscope itemtype="http://schema.org/WPHeader">
  <div class="slimContent">
    <div class="navbar-header">
      
      
      <div class="profile-block text-center">
        <a id="avatar" href="" target="_blank">
          <img class="img-circle img-rotate" src="/blog/images/avatar.jpg" width="200" height="200">
        </a>
        <h2 id="name" class="hidden-xs hidden-sm">emoairx</h2>
        <h3 id="title" class="hidden-xs hidden-sm hidden-md">PKU,EECS</h3>
        <small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Beijing, China</small>
      </div>
      
      <div class="search" id="search-form-wrap">

    <form class="search-form sidebar-form">
        <div class="input-group">
            <input type="text" class="search-form-input form-control" placeholder="Search" />
            <span class="input-group-btn">
                <button type="submit" class="search-form-submit btn btn-flat" onclick="return false;"><i class="icon icon-search"></i></button>
            </span>
        </div>
    </form>
    <div class="ins-search">
  <div class="ins-search-mask"></div>
  <div class="ins-search-container">
    <div class="ins-input-wrapper">
      <input type="text" class="ins-search-input" placeholder="Type something..." x-webkit-speech />
      <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
    </div>
    <div class="ins-section-wrapper">
      <div class="ins-section-container"></div>
    </div>
  </div>
</div>


</div>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
      <ul class="nav navbar-nav main-nav ">
        
        
        <li class="menu-item menu-item-home">
          <a href="/blog/.">
            
            <i class="icon icon-home-fill"></i>
            
            <span class="menu-title">Home</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-archives">
          <a href="/blog/archives">
            
            <i class="icon icon-archives-fill"></i>
            
            <span class="menu-title">Archives</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-links">
          <a href="/blog/links">
            
            <i class="icon icon-friendship"></i>
            
            <span class="menu-title">Links</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-about">
          <a href="/blog/about">
            
            <i class="icon icon-cup-fill"></i>
            
            <span class="menu-title">About</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-pkuinfo">
          <a href="/blog/info">
            
            <i class="icon "></i>
            
            <span class="menu-title">menu.PKUinfo</span>
          </a>
        </li>
        
      </ul>
      
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/emoairx" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
    </ul>

    </nav>
  </div>
</header>

  
    <aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    
      <div class="widget">
    <h3 class="widget-title">Board</h3>
    <div class="widget-body">
        <div id="board">
            <div class="content">
                <p>只是一些简单的生活分享吧</p>
            </div>
        </div>
    </div>
</div>

    
      

    
      
  <div class="widget">
    <h3 class="widget-title">Tags</h3>
    <div class="widget-body">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/ICS/" rel="tag">ICS</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/IG/" rel="tag">IG</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/NLP/" rel="tag">NLP</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/TCS%E5%9F%BA%E7%A1%80/" rel="tag">TCS基础</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/cs285/" rel="tag">cs285</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/%E4%BD%9C%E4%B8%9A/" rel="tag">作业</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/%E4%BF%A1%E6%81%AF%E8%AE%BA/" rel="tag">信息论</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/%E5%87%A0%E4%BD%95/" rel="tag">几何</a><span class="tag-list-count">3</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/%E5%87%B8%E4%BC%98%E5%8C%96/" rel="tag">凸优化</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/%E5%87%B8%E5%88%86%E6%9E%90%E4%B8%8E%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/" rel="tag">凸分析与优化方法</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/%E5%8D%9A%E5%BC%88%E8%AE%BA/" rel="tag">博弈论</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/%E5%9C%B0%E6%A6%82/" rel="tag">地概</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag">强化学习</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/%E6%97%A5%E8%AE%B0/" rel="tag">日记</a><span class="tag-list-count">86</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/%E6%9C%80%E4%BC%98%E4%BC%A0%E8%BE%93/" rel="tag">最优传输</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/%E6%B8%B8%E8%AE%B0/" rel="tag">游记</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/%E7%A8%8B%E8%AE%BE/" rel="tag">程设</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/%E7%AC%94%E8%AE%B0/" rel="tag">笔记</a><span class="tag-list-count">34</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/%E7%AE%97%E5%88%86/" rel="tag">算分</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/%E7%BB%8F%E5%8E%9F/" rel="tag">经原</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/%E9%9A%8F%E7%AC%94/" rel="tag">随笔</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/%E9%9F%B3%E6%95%B0/" rel="tag">音数</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget-body tagcloud">
      <a href="/blog/tags/ICS/" style="font-size: 13.2px;">ICS</a> <a href="/blog/tags/IG/" style="font-size: 13px;">IG</a> <a href="/blog/tags/NLP/" style="font-size: 13px;">NLP</a> <a href="/blog/tags/TCS%E5%9F%BA%E7%A1%80/" style="font-size: 13px;">TCS基础</a> <a href="/blog/tags/cs285/" style="font-size: 13px;">cs285</a> <a href="/blog/tags/%E4%BD%9C%E4%B8%9A/" style="font-size: 13.2px;">作业</a> <a href="/blog/tags/%E4%BF%A1%E6%81%AF%E8%AE%BA/" style="font-size: 13px;">信息论</a> <a href="/blog/tags/%E5%87%A0%E4%BD%95/" style="font-size: 13.4px;">几何</a> <a href="/blog/tags/%E5%87%B8%E4%BC%98%E5%8C%96/" style="font-size: 13px;">凸优化</a> <a href="/blog/tags/%E5%87%B8%E5%88%86%E6%9E%90%E4%B8%8E%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/" style="font-size: 13px;">凸分析与优化方法</a> <a href="/blog/tags/%E5%8D%9A%E5%BC%88%E8%AE%BA/" style="font-size: 13.2px;">博弈论</a> <a href="/blog/tags/%E5%9C%B0%E6%A6%82/" style="font-size: 13px;">地概</a> <a href="/blog/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" style="font-size: 13.6px;">强化学习</a> <a href="/blog/tags/%E6%97%A5%E8%AE%B0/" style="font-size: 14px;">日记</a> <a href="/blog/tags/%E6%9C%80%E4%BC%98%E4%BC%A0%E8%BE%93/" style="font-size: 13px;">最优传输</a> <a href="/blog/tags/%E6%B8%B8%E8%AE%B0/" style="font-size: 13.6px;">游记</a> <a href="/blog/tags/%E7%A8%8B%E8%AE%BE/" style="font-size: 13.2px;">程设</a> <a href="/blog/tags/%E7%AC%94%E8%AE%B0/" style="font-size: 13.8px;">笔记</a> <a href="/blog/tags/%E7%AE%97%E5%88%86/" style="font-size: 13px;">算分</a> <a href="/blog/tags/%E7%BB%8F%E5%8E%9F/" style="font-size: 13px;">经原</a> <a href="/blog/tags/%E9%9A%8F%E7%AC%94/" style="font-size: 13.6px;">随笔</a> <a href="/blog/tags/%E9%9F%B3%E6%95%B0/" style="font-size: 13px;">音数</a>
    </div>
  </div>

    
      
  <div class="widget">
    <h3 class="widget-title">Archive</h3>
    <div class="widget-body">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2024/12/">December 2024</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2024/11/">November 2024</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2024/10/">October 2024</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2024/09/">September 2024</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2024/08/">August 2024</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2024/07/">July 2024</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2024/06/">June 2024</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2024/05/">May 2024</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2024/04/">April 2024</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2024/02/">February 2024</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2024/01/">January 2024</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2023/12/">December 2023</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2023/10/">October 2023</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2023/08/">August 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2023/07/">July 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2023/05/">May 2023</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2023/04/">April 2023</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2023/03/">March 2023</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2023/02/">February 2023</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2023/01/">January 2023</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2022/12/">December 2022</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2022/11/">November 2022</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2022/10/">October 2022</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2022/09/">September 2022</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2022/08/">August 2022</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2022/07/">July 2022</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2022/06/">June 2022</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2022/05/">May 2022</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2022/04/">April 2022</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2022/03/">March 2022</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2022/01/">January 2022</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2021/12/">December 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2021/11/">November 2021</a><span class="archive-list-count">2</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget-body">
      <ul class="recent-post-list list-unstyled no-thumbnail">
        
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                
              </p>
              <p class="item-title">
                <a href="/blog/2024/12/05/diary/diary241205/" class="title">241205 - 摆烂人，优绩主义</a>
              </p>
              <p class="item-date">
                <time datetime="2024-12-05T12:47:56.374Z" itemprop="datePublished">2024-12-05</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                
              </p>
              <p class="item-title">
                <a href="/blog/2024/11/19/IG/" class="title">底层码农视角下的自然梯度下降法</a>
              </p>
              <p class="item-date">
                <time datetime="2024-11-19T05:04:49.972Z" itemprop="datePublished">2024-11-19</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                
              </p>
              <p class="item-title">
                <a href="/blog/2024/11/15/OT-2/" class="title">(no title)</a>
              </p>
              <p class="item-date">
                <time datetime="2024-11-15T13:38:26.501Z" itemprop="datePublished">2024-11-15</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                
              </p>
              <p class="item-title">
                <a href="/blog/2024/11/12/%E5%8A%B3%E5%8A%A8%E8%AF%BE%E5%B0%B1%E4%B8%9A%E7%AC%94%E8%AE%B0/" class="title">就业相关国内劳动法</a>
              </p>
              <p class="item-date">
                <time datetime="2024-11-12T11:08:19.142Z" itemprop="datePublished">2024-11-12</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                
              </p>
              <p class="item-title">
                <a href="/blog/2024/10/23/%E7%BC%96%E8%AF%91/" class="title">编译原理-理论</a>
              </p>
              <p class="item-date">
                <time datetime="2024-10-23T11:40:03.043Z" itemprop="datePublished">2024-10-23</time>
              </p>
            </div>
          </li>
          
      </ul>
    </div>
  </div>
  

    
  </div>
</aside>

  
  

  
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
  });
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


<main class="main" role="main">
  <div class="content">
  <article id="post-max entropy RL" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting">
    
    <div class="article-header">
      
        
  
    <h1 class="article-title" itemprop="name">
      最大熵强化学习——从概率图模型到SAC
    </h1>
  

      
      <div class="article-meta">
        <span class="article-date">
    <i class="icon icon-calendar-check"></i>
	<a href="/blog/2024/05/14/max%20entropy%20RL/" class="article-date">
	  <time datetime="2024-05-14T10:43:23.367Z" itemprop="datePublished">2024-05-14</time>
	</a>
</span>
        
        
  <span class="article-tag">
    <i class="icon icon-tags"></i>
	<a class="article-tag-link-link" href="/blog/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag">强化学习</a>, <a class="article-tag-link-link" href="/blog/tags/%E7%AC%94%E8%AE%B0/" rel="tag">笔记</a>
  </span>


        

	<span class="article-read hidden-xs">
    	<i class="icon icon-eye-fill" aria-hidden="true"></i>
    	<span id="/blog/2024/05/14/max%20entropy%20RL/" class="leancloud_visitors"  data-flag-title="最大熵强化学习——从概率图模型到SAC">
			<span class="leancloud-visitors-count">0</span>
		</span>
    </span>

        <span class="post-comment"><i class="icon icon-comment"></i> <a href="/blog/2024/05/14/max%20entropy%20RL/#comments" class="article-comment-link">Comments</a></span>
        
      </div>
    </div>
    <div class="article-entry marked-body" itemprop="articleBody">
      
        <p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/106698633">CS285 Lec14: Control as Inference Problem - 知乎 (zhihu.com)</a></p>
<p>这个讲得是真的好</p>
<p><img src="https://cdn.luogu.com.cn/upload/image_hosting/zs0zum8g.png" alt></p>
<p>我们最初有的设定是</p>
<script type="math/tex; mode=display">
p(O_t\mid s_t,a_t)\propto \exp (r(s_t,a_t))</script><p>对于给定的一张概率图</p>
<script type="math/tex; mode=display">
\begin{align*}
\beta_t(s_t,a_t)&=p(O_{t:T}\mid s_t,a_t)\\
&=\int p(O_{t:T},s_{t+1}\mid s_t,a_t) \mathrm d s_{t+1}\\
&=\int p(O_{t+1:T}\mid s_{t+1})p(s_{t+1}\mid s_t,a_t)p(O_t\mid s_t,a_t) \mathrm d s_{t+1}
\end{align*}</script><script type="math/tex; mode=display">
\beta_t(s_{t+1})=p(O_{t+1:T}\mid s_{t+1})=\int  p(O_{t+1:T}\mid {s_{t+1},a_{t+1}})p(a_{t+1}\mid s_{t+1})\mathrm d a_{t+1}</script><script type="math/tex; mode=display">
\beta_{t}(s_t,a_t)=p(O_t\mid s_t,a_t)\mathbb E_{s_{t+1}\sim p(s_{t+1}\mid s_t,a_t)}[\beta_{t+1}(s_{t+1})]\\
\beta_{t}(s_t)=\mathbb E_{a_t \sim p(a_t\mid s_t)}[\beta_t(s_t,a_t)]</script><p>let $V_t(s_t)=\log \beta_t(s_t)$, $Q_t(s_t,a_t)=\log \beta_t(s_t,a_t)$</p>
<p>if We take $p(a_t\mid s_t)$ as uniform,</p>
<script type="math/tex; mode=display">
V_t(s_t) = \log \int \exp(Q_t(s_t,a_t))\mathrm d a_t\\
Q_t(s_t,a_t)=r(s_t,a_t)+\log \mathbb E[\exp V_{t+1}(s_{t+1})]</script><p>注意这里如果直接做的话，Q关于V是要log sum exp的，后面我们会看到能把这个去掉。</p>
<p>Then we consider if $p(a_t\mid s_t)$ as a prior distribution which is not uniform.</p>
<script type="math/tex; mode=display">
V_t(s_t)=\log \int \exp(Q_t(s_t,a_t)+\log p(a_t\mid s_t)) \mathrm d a_t\\
Q_t(s_t,a_t)=r(s_t,a_t)+\log \mathbb E_{s_{t+1}}[\exp V_{t+1}(s_{t+1})]</script><p>so let </p>
<script type="math/tex; mode=display">
\tilde Q(s_t,a_t)=r(s_t,a_t)+\log  p(s_t\mid a_t) + \log \mathbb E[\exp V(s_{t+1})]</script><p>Then</p>
<script type="math/tex; mode=display">
V(s_t)=\log \int \exp ({\tilde Q(s_t,a_t)})</script><p>so, the policy $\pi(a_t\mid s_t)$ which is </p>
<script type="math/tex; mode=display">
\begin{align*}
p(a_t\mid s_t,O_{1:T})&=\pi(a_t\mid s_t)\\
&=\frac{p(a_t,s_t\mid O_{t:T})}{p(s_t\mid O_{t:T})}\\
&=\frac{p(O_{t:T}\mid a_t,s_t)p(s_t,a_t)/p(O_{t:T})}{p(O_{t:T}\mid s_t)p(s_t)/p(O_{t:T})}\\&=\frac{p(O_{t:T}\mid a_t,s_t)p(s_t,a_t)}{p(O_{t:T}\mid s_t)p(s_t)}\\
&=\frac{\beta_t(s_t,a_t)}{\beta_t(s_t)}p(a_t\mid s_t)\\
&=  \frac{\beta_t(s_t,a_t)}{\beta_t(s_t)}
\end{align*}</script><p>so we have</p>
<script type="math/tex; mode=display">
\pi(a_t\mid s_t)=\exp(Q_t(s_t,a_t)-V_t(s_t))=\exp A_t(s_t,a_t)</script><p>and temperature are allow to add.</p>
<p>Then we consider the <strong>optimism problem</strong>, for that </p>
<script type="math/tex; mode=display">
Q_t(s_t,a_t)=r(s_t,a_t)=\log E(\exp(V_{t+1}(s_{t+1})))</script><p>我们再回过头来回顾一下这些变量都代表什么意思：</p>
<p>$p(O_{t:T}\mid s_t,a_t)$：现在处于状态 $s_t$，选了方法 $a_t$，接下来能获得的得分的期望</p>
<p>$p(a<em>t\mid s_t,O</em>{1:T})$：为了获得最大的得分，我在当前状态的期望是什么，也就是当前的policy</p>
<p>$p(s<em>t\mid O</em>{1:{t-1}})$ </p>
<p>$p(s<em>{t+1}\mid s_t,a_t,O</em>{1:T})$ 是如果能获得最大的分数，那么转移的概率是什么</p>
<p>$p(s_{t+1}\mid s_t,a_t)$ 是转移的概率，和上面一个不一样。</p>
<script type="math/tex; mode=display">
\begin{align*}
p(\tau\mid O_{1:T})\propto&\ 
p(\tau,O_{1:T})\\
=&p(s_1)\prod _{t=1}^T p(O_{T}\mid s_t,a_t) p(s_{t+1}\mid s_t,a_t)
\\
=&p(s_1)\prod _{t=1}^T \exp (r(s_t,a_t)) p(s_{t+1}\mid s_t,a_t)
\\
=&\left[p(s_1)\prod_{t=1}^Tp(s_{t+1}\mid s_t,a_t)\right] \exp ({\sum_{i=1}^T r(s_t,a_t)})
\end{align*}</script><p>下面简记 $p(\tau) = p(\tau \mid O_{1:T})$</p>
<p>在这里，如果<strong>我们先假设</strong> </p>
<script type="math/tex; mode=display">
p(s_{t+1}\mid s_t,a_t,O_{1:T})=p(s_{t+1}\mid s_t,a_t)</script><p>，然后做如下推导</p>
<p>在给定最优变量 $O_{1:T}=1$ 的时候，整个概率图的轨迹分布可以表示为 $p(\tau)$ </p>
<p>而给定策略 $\pi(a_t\mid s_t)$ 的时候，整条轨迹的分布表示为</p>
<script type="math/tex; mode=display">
\hat p(\tau)=p(s_1 \mid O_{1:T})\prod p(s_{t+1}\mid s_t,a_t,O_{1:T}) \pi(a_t\mid s_t)
\\
\approx p(s_1)\prod p(s_{t+1}\mid s_t,a_t) \pi(a_t\mid s_t)</script><p>最小化两者的KL散度，也就是<strong>最大化</strong>负的散度</p>
<script type="math/tex; mode=display">
\begin{align*}
-D_{KL}(\hat p(\tau)||p(\tau))=&\mathbb E_{\tau \in \hat p(\tau)}
[
\log (p(s_1))+\sum_{t=1}^T(\log p(s_{t+1}\mid s_t,a_t)+r(s_t,a_t))\\
    &-\log (p(s_1))-\sum_{t=1}^T(\log p(s_{t+1}\mid s_t,a_t)+\log \pi(s_t,a_t))
]\\
=&\mathbb E_{\tau \in \hat p(\tau)}\left[\sum_{t=1}^T (r(s_t,a_t)-\log \pi(a_t\mid s_t)\right]\\
=&\sum_{t=1}^T \mathbb E_{(s_t,a_t) \sim \hat p(s_t,a_t)} [r(s_t,a_t)]+\mathbb E_{s_t \sim \hat p(s_t)} [\mathcal H(\pi(a_t\mid s_t))]
\end{align*}</script><blockquote>
<p>也就是按当前策略的方法去采样能获得的reward的情况。</p>
</blockquote>
<p>如果去掉 $p(s<em>{t+1}\mid s_t,a_t,O</em>{1:T})=p(s_{t+1}\mid s_t,a_t)$，这个假设，那么应该有</p>
<script type="math/tex; mode=display">
\begin{align*}
\hat p(\tau)=&p(s_1\mid O_{1:T}) \prod _{t=1}^T p(s_{t+1}\mid s_t,a_t,O_{1:T})p(a_t\mid s_t,O_{1:T})\\
\end{align*}</script><p>而</p>
<script type="math/tex; mode=display">
p(\tau)=\left[p(s_1)\prod_{t=1}^Tp(s_{t+1}\mid s_t,a_t)\right] \exp ({\sum_{i=1}^T r(s_t,a_t)})</script><blockquote>
<p>这里的 p(tau) 意味着在后验分布的情况下的，</p>
</blockquote>
<p>这两项之间如果还是求 <code>KL</code> 散度</p>
<script type="math/tex; mode=display">
\log p(s_1)-\log p(s_1\mid O_{1:T})+\sum_{t=1}^T\log p(s_{t+1}\mid s_t,a_t)\\-\sum_{t=1}^T \log p(s_{t+1}\mid s_t,a_t,O_{1:T}) + \sum_{t=1}^T (r(s_t,a_t)-\log \pi(a_t\mid s_t    ))\\</script><blockquote>
<p>据说是不太好优化的，我没具体搞懂</p>
</blockquote>
<p>我们考虑变分推断，用一个简单的变分函数去近似后验分布</p>
<script type="math/tex; mode=display">
p(\tau)=\left[p(s_1)\prod _{t=1}^T p(s_{t+1}\mid s_t,a_t)\right]\exp (\sum_{t=1}^T{r(s_t,a_t)})</script><p>这里用一个函数 $q$</p>
<script type="math/tex; mode=display">
q(\tau)=q(s_1) \prod_{t=1}^T q(s_{t+1}\mid s_t,a_t) q(a_t\mid s_t)</script><p>如果我们假设 $x=O<em>{1:T}, z=(s</em>{1:T},a_{1:T})$，那么可以理解成我们想要的分布是 $p(z\mid x)$，我们想用分布 $q(z)$ 来逼近它。</p>
<p><img src="https://cdn.luogu.com.cn/upload/image_hosting/ygmz2jdc.png" alt></p>
<script type="math/tex; mode=display">
\begin{align*}
\log p(x) =&\log \int_{z} p(x,z)\\
=&\log \int_z p(x,z)\frac{q(z)}{q(z)}\\
=&\log \mathbb E_{q} \left[\frac{p(x,z)}{q(z)}\right]\\
\ge& E_{z\sim q(z)}[\log p(x,z)-\log q(z)]
\end{align*}</script><script type="math/tex; mode=display">
\log p(\tau) \ge \mathbb E_{(s_{1:T},a_{1:T})\sim q}</script><p>我们在这里给 $q$ 设置一个约束条件，使得 $q(s<em>1)=p(s_1),q(s</em>{t+1}\mid s<em>t,a_t)=p(s</em>{t+1}\mid s_t,a_t)$ 成立，那么就能有</p>
<script type="math/tex; mode=display">
q(\tau)=p(s_1) \prod_{t=1}^T p(s_{t+1}\mid s_t,a_t) q(a_t\mid s_t)</script><p>于是</p>
<script type="math/tex; mode=display">
\log p(O_{1:T})\ge
\mathbb E_{(s_t,a_t) \sim q(s_t,a_t)}\left[\sum_{t=1}^T (r(s_t,a_t)-\log q(a_t\mid s_t)\right]\\</script><p>在上文中我们记</p>
<script type="math/tex; mode=display">
L=\mathbb E_q[\log p(x,z)]+\mathcal H(z)</script><p>这里做一个补充，为什么我们需要 $\log p(x)$，考虑一个KL散度</p>
<script type="math/tex; mode=display">
\begin{align} 
K L[q(Z) \| p(Z | X)] &=\int_{Z} q(Z) \log \frac{q(Z)}{p(Z | X)} \tag{6}\\ 
&=-\int_{Z} q(Z) \log \frac{p(Z | X)}{q(Z)} \tag{7}\\ 
&=-\left(\int_{Z} q(Z) \log \frac{p(X, Z)}{q(Z)}-\int_{Z} q(Z) \log p(X)\right) \tag{8}\\ 
&=-\int_{Z} q(Z) \log \frac{p(X, Z)}{q(Z)}+\log p(X) \int_{Z} q(Z) \tag{9}\\ 
&=-L+\log p(X) \tag{10}
\end{align}</script><p>上文中我们有</p>
<script type="math/tex; mode=display">
L=\log p(X)-K L[q(Z) \| p(Z | X)]</script><p>就是说如果估计分布和真实后验分布完美接近，那么这个 $L$ 就等于 $\log p(x)$。</p>
<p>我们型要让KL等于零，那么这里的log是多少？</p>
<p>所以我要优化这个KL尽可能地小，在 $\log p(x)$ 不变的情况下，我就去优化这个 $L$ 尽可能大。也就是这里的 $L$ 就变成了目标函数。天然地多出来一个 $\mathcal H(z)$</p>
<p>接下来我们呢继续推导 $q(a_T\mid s_T)$ 长什么样子</p>
<script type="math/tex; mode=display">
q(a_T\mid s_T)=\arg \max \mathbb E_{s_T\sim q(s_T),a_T\sim q(a_T\mid s_T)}\left[r(s_T,a_T)-\log q(a_T\mid s_T)\right]</script><p>可以解出来</p>
<script type="math/tex; mode=display">
q(a_T\mid s_T)\propto\ \exp(r(s_T,a_T))</script><p>也就是说，对于最后一个步骤</p>
<script type="math/tex; mode=display">
q(a_T\mid S_T)=\frac{\exp r(s_T,a_T)}{\int \exp (r(s_T,a))\mathrm da}=\exp(Q(s_T,a_T)-V(s_T))</script><p>这里设</p>
<script type="math/tex; mode=display">
V(S_T)=\log \int \exp(Q(s_T,a_T))\mathrm d a_T</script><p>同理得到对于中间的步骤</p>
<script type="math/tex; mode=display">
q(a_t\mid s_t)=\exp(Q(s_t,a_t)-V(s_t))</script><p><strong>注意这里我们是从零开始的</strong>，也就是我们是有了 $q$ 的正比这个性质，从而定义了 $Q$，</p>
<script type="math/tex; mode=display">
Q(s_t,a_t)=r(s_t,a_t)+E[V_{t+1}(s_{t+1})]</script><p>这里没有logsumexp。</p>
<p>在这个基础上，我们试着得出 <strong>soft Q-learning</strong></p>
<p>首先先看Q-learning with soft optimality，应该是这样的</p>
<script type="math/tex; mode=display">
\phi\leftarrow \phi+\alpha \nabla _{\phi}Q_\phi(s,a)(r(s,a)+\gamma V(s')-Q_\phi(s,a))</script><p>这里target value</p>
<script type="math/tex; mode=display">
V(s')=\text{soft}\max _{a'} Q_\phi(s',a')=\log \int \exp(Q_\phi(s',a'))da'</script><p>最后有</p>
<script type="math/tex; mode=display">
\pi(a\mid s)=\exp (Q_\phi(s,a)-V(s))=\exp (A(s,a))</script><p>我们考虑 soft Q learning的推导</p>
<p><strong>Reinforcement Learning with Deep Energy-Based Policies</strong></p>
<p>如果我们引入熵的话，我们想要有</p>
<script type="math/tex; mode=display">
\pi_{MaxEnt}^*=\arg \max_\pi \sum_{t=0} \mathbb E_{\rho_\pi}[\gamma^t(r_t+\alpha \mathcal H(\pi(\cdot \mid s_t)))]</script><p>在给定一个 $\pi$ 之后定义</p>
<script type="math/tex; mode=display">
Q_{soft}^\pi(s,a)\triangleq r(s_0,a_0)\mid_{s_0=s,a_0=a} + \mathbb E_{\tau \sim \pi}[\sum_{t=1}^\infty \gamma^t(r_t+\alpha \mathcal H(\pi(\cdot\mid s_t)))]\mid _{s_0=s,a_0=a}</script><p>那么最大熵策略的目标函数是</p>
<script type="math/tex; mode=display">
J(\pi)=\sum_t \mathbb E_{(s_t,a_t)\sim \rho_\pi}[Q_{soft}^\pi(s_t,a_t)+\alpha \mathcal H(\pi(\cdot \mid s_t))]</script><p>再来一个定义是</p>
<script type="math/tex; mode=display">
V_{soft}^\pi(s)\triangleq\alpha \log \int_A \exp(\frac{1}{\alpha}Q_{soft}^\pi{(s,a)})da\\
=\alpha \log \mathbb E_{q_a} \exp(\frac{1}{\alpha}Q_{soft}^\pi{(s,a)}-\log {q_a(a)})da</script><p>能够证明</p>
<script type="math/tex; mode=display">
Q_{soft}^\pi(s,a)=r(s,a)+\gamma \mathbb E_{s'}[V^\pi_{soft}(s')]</script><p>在这个基础上可以导出一个最优的policy</p>
<script type="math/tex; mode=display">
\tilde \pi(a\mid s)=\exp (\frac{1}{\alpha}({Q_{soft}^\pi(s,a)-V_{soft}^\pi(s))})</script><p>注意，这一项就是等号，不是一个正比号。</p>
<p>然后一个性质就是可以证明</p>
<script type="math/tex; mode=display">
Q_{soft}^\tilde \pi(s,\cdot )\ge Q_{soft}^\pi(s,\cdot )</script><p><strong>暂且略去这个证明</strong>。</p>
<p>总之我们也有了单调递增性。</p>
<blockquote>
<p><strong>entropy-regularized policy gradient can be viewed as performing soft Q-learning on the maximum-entropy.</strong></p>
</blockquote>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/32448644">漫谈Entropy, Energy and Learning. - 知乎 (zhihu.com)</a></p>
<p>这里另一种方法是是基于能量的模型 EBM</p>
<script type="math/tex; mode=display">
\max_p \mathbb E_{x\sim p}[\phi(x)]+\alpha \mathcal H(p)</script><p>它的最优分布是玻尔兹曼分布</p>
<script type="math/tex; mode=display">
p^*(x)\propto\ e^{-\phi(x)/\alpha}</script><p>这里的 $\alpha$ 是一个温度的系数。</p>
<script type="math/tex; mode=display">
\mathbb E_{x\sim p}[\frac{\phi(x)}{\alpha}-\log p(x)]=-KL(p||p_\phi)</script><p>这里 $p_\phi(x)=\frac{e^{\phi(x)/\alpha}}{Z}=\frac{e^{-\mathcal E(x)}}{Z}$，其中 $\mathcal E(x)=-\phi(x)/\alpha$。</p>
<p>能量越低越容易保持在这样一个状态，能量越高越容易一晃而过。</p>
<p>从这个角度我们来看策略优化，在某个状态下去找</p>
<script type="math/tex; mode=display">
\text{soft} \max_a \mathbb E_{a\sim \pi(\cdot \mid s)}[Q(s,a)] + \alpha \mathcal H(\pi(\cdot \mid s))</script><p>就有</p>
<script type="math/tex; mode=display">
\pi^*(a\mid s)\propto \exp(Q(s,a)/\alpha)</script><p>从上文我们定义了soft的V和Q，我们有</p>
<script type="math/tex; mode=display">
V^\pi_{soft}(s_t)=\mathbb E_{a\sim \pi}[Q^\pi_{soft}(s_t,a_t)]+\alpha \mathcal H(\pi(\cdot \mid s_t))
\\Q_{soft}^\pi(s,a)=r(s,a)+\gamma \mathbb E_{s'}[V^\pi_{soft}(s')]</script><p>接着我们来说一下soft Q-learning，首先是去学习（更新）<code>Qsoft</code></p>
<script type="math/tex; mode=display">
J_Q(\theta)=\mathbb E_{(s_t,a_t)}[\frac{1}{2}(r(s,a)+\gamma \mathbb E_{s'}[V^{\bar \theta}_{soft}(s')-Q^\theta_{soft}(s_t,a_t))^2]</script><p>因为希望通过当前的policy对Q更新， $V$ 要做一步重要性采样来算出来，前文提到的</p>
<script type="math/tex; mode=display">
V_{soft}^\theta(s_t)=\alpha \log \mathbb E_{\pi} 
\frac{
\exp(\frac{1}{\alpha}Q_{soft}^{\theta}{(s_t,a')})
}{\pi(a')}</script><p>但是 $\pi$ 还是不能直接通过贪心来求。</p>
<p>所以在 soft Q-learning 里面，通过最小化当前policy和基于价值函数构造出来的soft max分布的形式，用 SVGD 通过KL散度优化策略。（对于连续空间）</p>
<p><strong>soft Actor-Critic</strong> 希望直接去优化策略</p>
<p>前文提到</p>
<script type="math/tex; mode=display">
\pi_{new}=\arg \min_{\pi'} D_{KL}(\pi'||\exp(Q^{\pi}old)/Z)</script><p>策略迭代能保证迭代之后的价值不小于迭代之前的价值。</p>
<p>所以这个方法的算法的细节包括如下两种</p>
<p>首先通过bellman方程迭代更新Q和V，使用两个Q网络，$\bar \psi=\tau \psi +(1-\tau)\bar \psi$。</p>
<p>还有一个策略函数的学习目标，通过正比于 $Q+\mathcal H$ 的方法，这是我们之前用概率图方法推导出来的。</p>
<script type="math/tex; mode=display">
J_\pi(\phi)=\mathbb E_{s_t\sim D,\epsilon _t\sim N}[\log \pi_\phi(f_\phi(\epsilon_t;s_t)\mid s_t)-Q_\theta(s_t,f_\phi(\epsilon_t;s_t))]</script><p>所以迭代更新。</p>
<p>实验：</p>
<p>认为它非常diverse，确实是鼓励了探索。</p>
<p>softQ作为预训练，对于不同的环境，也更好。</p>
<p>SAC认为策略用随机策略比固定策略更好，虽然一开始比较慢，但是最终会好。但是估值的时候相比于随机策略，用固定策略的更好。</p>
<hr>
<p>我们来回顾一下这个 <code>entropy</code> 到底是哪里出现的。</p>
<p>最开始的目标函数中出现是因为我们用了变分不等式。</p>
<p>然后是值迭代的时候，$V$ 用 $\log \int \exp$ 来估计，这个 $\int$ 是平均意义上的，对于特定的策略的话就需要一个熵。</p>

      
    </div>
    <div class="article-footer">
      <blockquote class="mt-2x">
  <ul class="post-copyright list-unstyled">
    
    <li class="post-copyright-link hidden-xs">
      <strong>本文链接：</strong>
      <a href="http://emoairx.github.io/blog/2024/05/14/max%20entropy%20RL/" title="最大熵强化学习——从概率图模型到SAC" target="_blank" rel="external">http://emoairx.github.io/blog/2024/05/14/max entropy RL/</a>
    </li>
    
    <li class="post-copyright-license">
      <strong>版权声明： </strong> 本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！
    </li>
  </ul>
</blockquote>


<div class="panel panel-default panel-badger">
  <div class="panel-body">
    <figure class="media">
      <div class="media-left">
        <a href="" target="_blank" class="img-burn thumb-sm visible-lg">
          <img src="/blog/images/avatar.jpg" class="img-rounded w-full" alt="">
        </a>
      </div>
      <div class="media-body">
        <h3 class="media-heading"><a href="" target="_blank"><span class="text-dark">emoairx</span><small class="ml-1x">PKU,EECS</small></a></h3>
        <div>春天来了，冬天还会远吗~</div>
      </div>
    </figure>
  </div>
</div>


    </div>
  </article>
  
    
  <section id="comments">
  	
      <div id="vcomments"></div>
    
  </section>


  
</div>

  <nav class="bar bar-footer clearfix" data-stick-bottom>
  <div class="bar-inner">
  
  <ul class="pager pull-left">
    
    <li class="prev">
      <a href="/blog/2024/05/23/deepspeed/" title="(no title)"><i class="icon icon-angle-left" aria-hidden="true"></i><span>&nbsp;&nbsp;Newer</span></a>
    </li>
    
    
    <li class="next">
      <a href="/blog/2024/05/14/note%20on%20Information%20Theory/" title="Information Theory - wll"><span>Older&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a>
    </li>
    
    
  </ul>
  
  
  
  <div class="bar-right">
    
  </div>
  </div>
</nav>
  


</main>

  <footer class="footer" itemscope itemtype="http://schema.org/WPFooter">
	
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/emoairx" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
    </ul>

    <div class="copyright">
    	
        <div class="publishby">
        	Theme by <a href="https://github.com/cofess" target="_blank"> cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>.
        </div>
    </div>
</footer>
  <script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
<script>
window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')
</script>

<script src="/blog/js/plugin.min.js"></script>


<script src="/blog/js/application.js"></script>


    <script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/blog/',
        CONTENT_URL: '/blog/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/blog/js/insight.js"></script>






   




   
    
  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/valine"></script>
  <script type="text/javascript">
  var GUEST = ['nick', 'mail', 'link'];
  var meta = 'nick,mail,link';
  meta = meta.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#vcomments',
    verify: false,
    notify: false,
    appId: 'AONthdWWfk53xtyouAX8NhIf-gzGzoHsz',
    appKey: '7lxwWP9F0g0a68t0qEPsTFR9',
    placeholder: 'Just go go',
    avatar: 'mm',
    meta: meta,
    pageSize: '10' || 10,
    visitor: true
  });
  </script>

     







</body>
</html>