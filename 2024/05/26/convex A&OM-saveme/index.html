<!DOCTYPE html>
<html lang=en>
<head>
  <meta charset="utf-8">
  
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, minimum-scale=1, user-scalable=no, minimal-ui">
  <meta name="renderer" content="webkit">
  <meta http-equiv="Cache-Control" content="no-transform" />
  <meta http-equiv="Cache-Control" content="no-siteapp" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <meta name="format-detection" content="telephone=no,email=no,adress=no">
  <!-- Color theme for statusbar -->
  <meta name="theme-color" content="#000000" />
  <!-- 强制页面在当前窗口以独立页面显示,防止别人在框架里调用页面 -->
  <meta http-equiv="window-target" content="_top" />
  
  
  <title>凸分析与优化方法笔记 | emoairx</title>
  <meta name="description" content="本课又名林教授的人生哲理和他十五年前的工作，本笔记不包含人生哲理部分。 依然是学不会也没花太多时间学的一门课，靠，就是这种什么都不会，慌的一批的感觉。 学疯了，第五章第六章第七章第八章真的完全学不会。 草我不会是个傻子吧，怎么什么都不会啊。 我不会真是个傻子吧 Chapter1: Overview这一节不是很重要  \min_{x \in R^n} \frac{1}{m} f_i(x) + \la">
<meta property="og:type" content="article">
<meta property="og:title" content="凸分析与优化方法笔记">
<meta property="og:url" content="http://emoairx.github.io/blog/2024/05/26/convex%20A&OM-saveme/index.html">
<meta property="og:site_name" content="Emoairx">
<meta property="og:description" content="本课又名林教授的人生哲理和他十五年前的工作，本笔记不包含人生哲理部分。 依然是学不会也没花太多时间学的一门课，靠，就是这种什么都不会，慌的一批的感觉。 学疯了，第五章第六章第七章第八章真的完全学不会。 草我不会是个傻子吧，怎么什么都不会啊。 我不会真是个傻子吧 Chapter1: Overview这一节不是很重要  \min_{x \in R^n} \frac{1}{m} f_i(x) + \la">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://cdn.luogu.com.cn/upload/image_hosting/itmzw9y5.png">
<meta property="article:published_time" content="2024-05-26T08:21:03.870Z">
<meta property="article:modified_time" content="2024-06-11T15:07:59.932Z">
<meta property="article:author" content="emoairx">
<meta property="article:tag" content="笔记">
<meta property="article:tag" content="凸分析与优化方法">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.luogu.com.cn/upload/image_hosting/itmzw9y5.png">
  <!-- Canonical links -->
  <link rel="canonical" href="http://emoairx.github.io/blog/2024/05/26/convex%20A&OM-saveme/index.html">
  
    <link rel="alternate" href="/atom.xml" title="Emoairx" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png" type="image/x-icon">
  
  
<link rel="stylesheet" href="/blog/css/style.css">

  
  
  
  
<meta name="generator" content="Hexo 5.4.0"></head>


<body class="main-center" itemscope itemtype="http://schema.org/WebPage">
  <header class="header" itemscope itemtype="http://schema.org/WPHeader">
  <div class="slimContent">
    <div class="navbar-header">
      
      
      <div class="profile-block text-center">
        <a id="avatar" href="" target="_blank">
          <img class="img-circle img-rotate" src="/blog/images/avatar.jpg" width="200" height="200">
        </a>
        <h2 id="name" class="hidden-xs hidden-sm">emoairx</h2>
        <h3 id="title" class="hidden-xs hidden-sm hidden-md">PKU,EECS</h3>
        <small id="location" class="text-muted hidden-xs hidden-sm"><i class="icon icon-map-marker"></i> Beijing, China</small>
      </div>
      
      <div class="search" id="search-form-wrap">

    <form class="search-form sidebar-form">
        <div class="input-group">
            <input type="text" class="search-form-input form-control" placeholder="Search" />
            <span class="input-group-btn">
                <button type="submit" class="search-form-submit btn btn-flat" onclick="return false;"><i class="icon icon-search"></i></button>
            </span>
        </div>
    </form>
    <div class="ins-search">
  <div class="ins-search-mask"></div>
  <div class="ins-search-container">
    <div class="ins-input-wrapper">
      <input type="text" class="ins-search-input" placeholder="Type something..." x-webkit-speech />
      <button type="button" class="close ins-close ins-selectable" data-dismiss="modal" aria-label="Close"><span aria-hidden="true">×</span></button>
    </div>
    <div class="ins-section-wrapper">
      <div class="ins-section-container"></div>
    </div>
  </div>
</div>


</div>
      <button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#main-navbar" aria-controls="main-navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
    </div>
    <nav id="main-navbar" class="collapse navbar-collapse" itemscope itemtype="http://schema.org/SiteNavigationElement" role="navigation">
      <ul class="nav navbar-nav main-nav ">
        
        
        <li class="menu-item menu-item-home">
          <a href="/blog/.">
            
            <i class="icon icon-home-fill"></i>
            
            <span class="menu-title">Home</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-archives">
          <a href="/blog/archives">
            
            <i class="icon icon-archives-fill"></i>
            
            <span class="menu-title">Archives</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-links">
          <a href="/blog/links">
            
            <i class="icon icon-friendship"></i>
            
            <span class="menu-title">Links</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-about">
          <a href="/blog/about">
            
            <i class="icon icon-cup-fill"></i>
            
            <span class="menu-title">About</span>
          </a>
        </li>
        
        
        <li class="menu-item menu-item-pkuinfo">
          <a href="/blog/info">
            
            <i class="icon "></i>
            
            <span class="menu-title">menu.PKUinfo</span>
          </a>
        </li>
        
      </ul>
      
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/emoairx" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
    </ul>

    </nav>
  </div>
</header>

  
    <aside class="sidebar" itemscope itemtype="http://schema.org/WPSideBar">
  <div class="slimContent">
    
      <div class="widget">
    <h3 class="widget-title">Board</h3>
    <div class="widget-body">
        <div id="board">
            <div class="content">
                <p>只是一些简单的生活分享吧</p>
            </div>
        </div>
    </div>
</div>

    
      

    
      
  <div class="widget">
    <h3 class="widget-title">Tags</h3>
    <div class="widget-body">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/ICS/" rel="tag">ICS</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/NLP/" rel="tag">NLP</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/TCS%E5%9F%BA%E7%A1%80/" rel="tag">TCS基础</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/cs285/" rel="tag">cs285</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/%E4%BD%9C%E4%B8%9A/" rel="tag">作业</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/%E4%BF%A1%E6%81%AF%E8%AE%BA/" rel="tag">信息论</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/%E5%87%B8%E4%BC%98%E5%8C%96/" rel="tag">凸优化</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/%E5%87%B8%E5%88%86%E6%9E%90%E4%B8%8E%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/" rel="tag">凸分析与优化方法</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/%E5%8D%9A%E5%BC%88%E8%AE%BA/" rel="tag">博弈论</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/%E5%9C%B0%E6%A6%82/" rel="tag">地概</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" rel="tag">强化学习</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/%E6%97%A5%E8%AE%B0/" rel="tag">日记</a><span class="tag-list-count">82</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/%E6%B8%B8%E8%AE%B0/" rel="tag">游记</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/%E7%A8%8B%E8%AE%BE/" rel="tag">程设</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/%E7%AC%94%E8%AE%B0/" rel="tag">笔记</a><span class="tag-list-count">23</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/%E7%AE%97%E5%88%86/" rel="tag">算分</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/%E7%BB%8F%E5%8E%9F/" rel="tag">经原</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/%E9%9A%8F%E7%AC%94/" rel="tag">随笔</a><span class="tag-list-count">4</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/%E9%9F%B3%E6%95%B0/" rel="tag">音数</a><span class="tag-list-count">1</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget-body tagcloud">
      <a href="/blog/tags/ICS/" style="font-size: 13.25px;">ICS</a> <a href="/blog/tags/NLP/" style="font-size: 13px;">NLP</a> <a href="/blog/tags/TCS%E5%9F%BA%E7%A1%80/" style="font-size: 13px;">TCS基础</a> <a href="/blog/tags/cs285/" style="font-size: 13px;">cs285</a> <a href="/blog/tags/%E4%BD%9C%E4%B8%9A/" style="font-size: 13.25px;">作业</a> <a href="/blog/tags/%E4%BF%A1%E6%81%AF%E8%AE%BA/" style="font-size: 13px;">信息论</a> <a href="/blog/tags/%E5%87%B8%E4%BC%98%E5%8C%96/" style="font-size: 13px;">凸优化</a> <a href="/blog/tags/%E5%87%B8%E5%88%86%E6%9E%90%E4%B8%8E%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/" style="font-size: 13px;">凸分析与优化方法</a> <a href="/blog/tags/%E5%8D%9A%E5%BC%88%E8%AE%BA/" style="font-size: 13.25px;">博弈论</a> <a href="/blog/tags/%E5%9C%B0%E6%A6%82/" style="font-size: 13px;">地概</a> <a href="/blog/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/" style="font-size: 13.5px;">强化学习</a> <a href="/blog/tags/%E6%97%A5%E8%AE%B0/" style="font-size: 14px;">日记</a> <a href="/blog/tags/%E6%B8%B8%E8%AE%B0/" style="font-size: 13.5px;">游记</a> <a href="/blog/tags/%E7%A8%8B%E8%AE%BE/" style="font-size: 13.25px;">程设</a> <a href="/blog/tags/%E7%AC%94%E8%AE%B0/" style="font-size: 13.75px;">笔记</a> <a href="/blog/tags/%E7%AE%97%E5%88%86/" style="font-size: 13px;">算分</a> <a href="/blog/tags/%E7%BB%8F%E5%8E%9F/" style="font-size: 13px;">经原</a> <a href="/blog/tags/%E9%9A%8F%E7%AC%94/" style="font-size: 13.5px;">随笔</a> <a href="/blog/tags/%E9%9F%B3%E6%95%B0/" style="font-size: 13px;">音数</a>
    </div>
  </div>

    
      
  <div class="widget">
    <h3 class="widget-title">Archive</h3>
    <div class="widget-body">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2024/06/">June 2024</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2024/05/">May 2024</a><span class="archive-list-count">9</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2024/04/">April 2024</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2024/02/">February 2024</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2024/01/">January 2024</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2023/12/">December 2023</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2023/10/">October 2023</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2023/08/">August 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2023/07/">July 2023</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2023/05/">May 2023</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2023/04/">April 2023</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2023/03/">March 2023</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2023/02/">February 2023</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2023/01/">January 2023</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2022/12/">December 2022</a><span class="archive-list-count">4</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2022/11/">November 2022</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2022/10/">October 2022</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2022/09/">September 2022</a><span class="archive-list-count">10</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2022/08/">August 2022</a><span class="archive-list-count">8</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2022/07/">July 2022</a><span class="archive-list-count">5</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2022/06/">June 2022</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2022/05/">May 2022</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2022/04/">April 2022</a><span class="archive-list-count">7</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2022/03/">March 2022</a><span class="archive-list-count">11</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2022/01/">January 2022</a><span class="archive-list-count">2</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2021/12/">December 2021</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2021/11/">November 2021</a><span class="archive-list-count">2</span></li></ul>
    </div>
  </div>


    
      
  <div class="widget">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget-body">
      <ul class="recent-post-list list-unstyled no-thumbnail">
        
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                
              </p>
              <p class="item-title">
                <a href="/blog/2024/06/15/nonsense/" class="title">nonsense</a>
              </p>
              <p class="item-date">
                <time datetime="2024-06-15T11:41:21.886Z" itemprop="datePublished">2024-06-15</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                
              </p>
              <p class="item-title">
                <a href="/blog/2024/06/14/diary/diary240613/" class="title">240613</a>
              </p>
              <p class="item-date">
                <time datetime="2024-06-14T14:53:18.206Z" itemprop="datePublished">2024-06-14</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                
              </p>
              <p class="item-title">
                <a href="/blog/2024/06/13/reward%20learning/" class="title">reward learning</a>
              </p>
              <p class="item-date">
                <time datetime="2024-06-13T09:18:40.754Z" itemprop="datePublished">2024-06-13</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                
              </p>
              <p class="item-title">
                <a href="/blog/2024/06/05/diary/diary240605/" class="title">240605</a>
              </p>
              <p class="item-date">
                <time datetime="2024-06-05T07:13:14.755Z" itemprop="datePublished">2024-06-05</time>
              </p>
            </div>
          </li>
          
          <li>
            
            <div class="item-inner">
              <p class="item-category">
                
              </p>
              <p class="item-title">
                <a href="/blog/2024/06/02/InverseRL/" class="title">Inverse Reinforcement Learning</a>
              </p>
              <p class="item-date">
                <time datetime="2024-06-02T07:56:28.276Z" itemprop="datePublished">2024-06-02</time>
              </p>
            </div>
          </li>
          
      </ul>
    </div>
  </div>
  

    
  </div>
</aside>

  
  

  
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
  });
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


<main class="main" role="main">
  <div class="content">
  <article id="post-convex A&amp;OM-saveme" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting">
    
    <div class="article-header">
      
        
  
    <h1 class="article-title" itemprop="name">
      凸分析与优化方法笔记
    </h1>
  

      
      <div class="article-meta">
        <span class="article-date">
    <i class="icon icon-calendar-check"></i>
	<a href="/blog/2024/05/26/convex%20A&OM-saveme/" class="article-date">
	  <time datetime="2024-05-26T08:21:03.870Z" itemprop="datePublished">2024-05-26</time>
	</a>
</span>
        
        
  <span class="article-tag">
    <i class="icon icon-tags"></i>
	<a class="article-tag-link-link" href="/blog/tags/%E5%87%B8%E5%88%86%E6%9E%90%E4%B8%8E%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95/" rel="tag">凸分析与优化方法</a>, <a class="article-tag-link-link" href="/blog/tags/%E7%AC%94%E8%AE%B0/" rel="tag">笔记</a>
  </span>


        

	<span class="article-read hidden-xs">
    	<i class="icon icon-eye-fill" aria-hidden="true"></i>
    	<span id="/blog/2024/05/26/convex%20A&OM-saveme/" class="leancloud_visitors"  data-flag-title="凸分析与优化方法笔记">
			<span class="leancloud-visitors-count">0</span>
		</span>
    </span>

        <span class="post-comment"><i class="icon icon-comment"></i> <a href="/blog/2024/05/26/convex%20A&OM-saveme/#comments" class="article-comment-link">Comments</a></span>
        
      </div>
    </div>
    <div class="article-entry marked-body" itemprop="articleBody">
      
        <p>本课又名林教授的人生哲理和他十五年前的工作，本笔记不包含人生哲理部分。</p>
<p>依然是学不会也没花太多时间学的一门课，靠，就是这种什么都不会，慌的一批的感觉。</p>
<p>学疯了，第五章第六章第七章第八章真的完全学不会。</p>
<p>草我不会是个傻子吧，怎么什么都不会啊。</p>
<p>我不会真是个傻子吧</p>
<h3 id="Chapter1-Overview"><a href="#Chapter1-Overview" class="headerlink" title="Chapter1: Overview"></a>Chapter1: Overview</h3><p>这一节不是很重要</p>
<script type="math/tex; mode=display">
\min_{x \in R^n} \frac{1}{m} f_i(x) + \lambda R(x)</script><blockquote>
<p>一定要写优化的变量，就是针对什么来优化的。</p>
</blockquote>
<p>例如，SVM的优化是</p>
<script type="math/tex; mode=display">
\min_{w,v} ||w||^2 , s.t.\ y_i(w^Tx_i+b) \ge 1</script><p>GAN的优化目标是</p>
<script type="math/tex; mode=display">
\min _G \max _D V(D,G) =\mathbb E _{x \sim p_{data}(x)} [\log D(x)] + E_{z \sim p_z(z)} [\log (1-D(G(z)))]</script><p>Adversarial Learning的目标是</p>
<script type="math/tex; mode=display">
\min_{\delta \in R^n} d(x,x+\delta), s.t. C(x+\delta)=c,x+\delta \in [0,1]^n</script><blockquote>
<p>噪声要小，效果要好</p>
</blockquote>
<p>Hyper parameter Search</p>
<script type="math/tex; mode=display">
\min_x F(x,y),\\s.t. x\in C_1,y\in\arg\min_{y\in C_2 } f(x,y)</script><p><strong>Machine Learning = Representation  + Optimization + Evaluation</strong> — Pedro Domingos</p>
<blockquote>
<p> P. Domingos. A few useful things to know about machine learning. Communications of the ACM,  55(10):78-87, 2012</p>
</blockquote>
<p>Is optimization difficult? </p>
<blockquote>
<p>NO! <a target="_blank" rel="noopener" href="https://blog.sina.com.cn/s/blog_649886d60102drfd.html">[转载]来自南京大学数学系张高飞老师<em>谢纳新</em>新浪博客 (sina.com.cn)</a></p>
<p>代数几何&gt;复分析、调和分析、微分方程&gt;几何&gt;动力系统&gt;组合数学&gt;统计&gt;计算数学</p>
<p>最后这一条是专门针对那些悲情人物的。他们连小学的数学也没学好。不要说 把上千个数加起来，就是把两个数加起来，对他们来说都是件很吃力的事。然 而这一切丝毫没有削弱他们对数学的一片痴情。他们日日夜夜泡在图书馆里。 他们翻阅了所有的数学文献，却从未找到一本能读懂的。 但他们仍坚持不懈，  为的就是找到一个适合自己的专业。他们的行为感动了上帝。上世纪的某一天， 上帝为他们创造了一台机器帮他们计算。这就是计算机。借助计算机，他们可 以很快地进行加减乘除的运算。这就是计算数学。</p>
</blockquote>
<p><strong>No-free-lunch Theorem for Opt.</strong></p>
<ul>
<li><p>If algorithm A performs better than algorithm B for some  optimization functions, then B will outperform A for other  functions. </p>
</li>
<li><blockquote>
<p>D. H. Wolpert and W. G. Macready, No free lunch theorems for optimization, IEEE T. Evolutionary  Computation, 1, 67-82 (1997).</p>
</blockquote>
</li>
</ul>
<p><strong>优化问题的分类</strong></p>
<p>the nature of the solution set</p>
<ul>
<li>Continuous opt. problems</li>
<li>Discrete opt. problems</li>
<li>Combinatorial opt. problems</li>
<li>Variational opt. problems </li>
</ul>
<p>the description (definition) of the solution set</p>
<ul>
<li>Unconstrained, Constrained(Equality constrained,Inequality constrained)</li>
</ul>
<p>the properties of the objective function </p>
<ul>
<li>Convex vs. nonconvex</li>
<li>convex programs:<ul>
<li>quadratic program，semi-definite programs，second-order cone programs</li>
<li>in ML, a lot of regression.</li>
</ul>
</li>
<li>nonconvex：要么是函数非凸，要么是约束域非凸。<ul>
<li>比如分数规划，minmax问题，spare models</li>
</ul>
</li>
</ul>
<hr>
<h3 id="Chapter2-math"><a href="#Chapter2-math" class="headerlink" title="Chapter2 - math"></a>Chapter2 - math</h3><p><strong>Open set</strong>：开集</p>
<p><strong>Closed set</strong>：闭集（补集是开集）</p>
<p><strong>bounded set</strong>：有界集，每个元素的范数不超过一个 $R$</p>
<p><strong>compact set</strong>：紧集，有界闭</p>
<p><strong>interior</strong>：内点集，$C^o={y\mid \exists \epsilon&gt;0,B_\epsilon(y)\sub C}$</p>
<p><strong>closure</strong>：闭包集，补集的内点 $\bar C$</p>
<p><strong>boundary</strong>：边界，$\bar C \backslash C^o$</p>
<p><strong>Global Convergence</strong>: 每个点都能收敛到FONC ($\nabla f(x^*)=0$) 条件的点。</p>
<p><strong>Local Convergence:</strong> 只有足够近才能收敛，否则不收敛（比如牛顿法）或者不FONC（比如鞍点）</p>
<p><strong>Convergence rate</strong>:</p>
<ul>
<li><p>$x_k \to x^*$</p>
</li>
<li><script type="math/tex; mode=display">
e_k=x_k-x^*</script></li>
<li></li>
<li><p>如果说 rate 为 $r$ ，rate constant为 $C$，说明</p>
<script type="math/tex; mode=display">
\lim_{k \to \infty} \frac{||e_{k+1}||}{||e_k||^r}=C, (C < \infty)</script></li>
<li><blockquote>
<p>显然是 r 越大越好， C 越小越好。</p>
</blockquote>
</li>
<li><p>线性收敛则 $r=1,0&lt;C&lt;1$，<strong>Q-linear</strong></p>
</li>
<li><p>sublinear则 $r=1,C=1$,  superlinear 则 $r=1,C=0$, Quadratic 则 $r=2$。r 可以不是整数。</p>
</li>
</ul>
<blockquote>
<p>比如 $x<em>k=0.99^k,x^*=0,e</em>{k+1}/e_k=0.99$ 这个就是线性收敛。</p>
</blockquote>
<ul>
<li>注意到 $\log ||e_{k+1}||\approx r \log ||e_k|| + \log C$，所以可以估计 $r$。</li>
</ul>
<blockquote>
<p>有时候不太能用 Q-linear收敛</p>
</blockquote>
<ul>
<li>如果 $||x_k-x^<em>|| \le e_k$ 且 $e_k$ 以 <strong>Q-linear</strong> 收敛到零，那么 ${x_k}$ 就以 <em>*R-linear</em></em> 收敛，</li>
</ul>
<p><strong>closedness</strong>(函数的)</p>
<ul>
<li>函数 $f$ 是闭的，如果对于所有 $\alpha$ 有 $\set{x \in dom f\mid f(x) \le \alpha}$ 是闭的。</li>
<li><p>也就是 $\text{epi} f={(x,t) \in R^{n+1} \mid x \in dom f,f(x) \le t}$ 是闭的。</p>
</li>
<li><p>连续函数只要定义域是闭的就是闭的，如果定义域是开的，那么到从内点到边界点的一个收敛序列的函数值必须是趋向于无穷。</p>
</li>
</ul>
<blockquote>
<p>$f(x)=\cases{x\log x&amp; x&gt;0\0&amp;x=0}$ 是闭的， $f(x)=-\log x ,x&gt;0$ 是闭的。$f(x)=x\log x,x&gt;0$ 是开的</p>
</blockquote>
<p><strong>Derivative</strong>(函数的)</p>
<ul>
<li>对于函数 $f:R^n \to R^m$</li>
<li>定义m*n 的 Jacobian矩阵 $J$ 的第 i 列为 $\frac{\partial f(x)}{\partial x_i}$。</li>
</ul>
<script type="math/tex; mode=display">
\lim _{z\to x,z\not=x,z\in domf} \frac{||f(z)-f(x)-J(z-x)||}{||z-x||^2} =0</script><p><strong>Gradient</strong></p>
<ul>
<li><script type="math/tex; mode=display">
\nabla f(x)=Df(x)^T</script></li>
</ul>
<script type="math/tex; mode=display">
f(x)=\log \det X,f =\mathbb S^n_{++}</script><p>let $Z$ be close to $X$, $\Delta X=Z-X$ , $\lambda_i$ be the ith eigenvalue of $X^{-1/2}\Delta X X^{-1/2}$</p>
<script type="math/tex; mode=display">
\begin{align*}
\log \det Z&=\log \det (X+\Delta X)\\
&=\log \det (X^{1/2}(I+X^{-1/2}\Delta X X^{-1/2})X^{1/2})\\
&=\log \det X+ \log \det (I+X^{-1/2}\Delta XX^{-1/2})\\
&=\log \det X+\sum_{i=1}^n \log (1+\lambda_i)\\
&\approx\log \det X+\sum_{i=1}^n \lambda_i\\
&=\log \det X+\tr(X^{-1}\Delta X)\\
&=\log \det X+\tr(X^{-1}(Z-X))\\
\end{align*}</script><p><strong>Chain rule</strong></p>
<script type="math/tex; mode=display">
A:R^{n*p},b:R^n,g:R^p\to R^m,g(x)=f(Ax+b)</script><p>when f is a real-valued</p>
<script type="math/tex; mode=display">
Dg(x)=Df(Ax+b)A\\\nabla g(x)=A^T\nabla f(Ax+b)</script><blockquote>
<p>lets have a try at </p>
<script type="math/tex; mode=display">
f(x)=\log \sum_{i=1}^m \exp(a_i^T x+b_i)</script></blockquote>
<p><strong>Second derivative</strong></p>
<script type="math/tex; mode=display">
D^2f(x)=\mathrm H</script><p>H means Hessian.</p>
<script type="math/tex; mode=display">
f(z)=f(x)+\nabla f(x)^T(z-x)+\frac{1}{2}(z-x)^T \nabla^2f(x) (z-x)\\
=Df(x)(z-x)+\frac{1}{2}\left<D^2f(x)(z-x),z-x\right></script><blockquote>
<p>so lets have a try of $f(x)=\log \det X$</p>
</blockquote>
<script type="math/tex; mode=display">
\log \det Z = \log \det X+\sum_{i=1}^n \log (1+\lambda_i) \\\approx \log \det X + \sum \lambda_i - \frac{1}{2}\sum \lambda_i^2</script><p><strong>Chain rules for second derivative</strong></p>
<ul>
<li><p>for $g(x)=f(Ax+b)$</p>
</li>
<li><script type="math/tex; mode=display">
\nabla^2 g(x)=A^T \nabla^2 f(Ax+b) A</script></li>
</ul>
<p><strong>Reparameterization trick</strong></p>
<script type="math/tex; mode=display">
L(\theta)= \mathbb E_{z\sim p_\theta(z)} [f(z)]</script><p><strong>SVD</strong></p>
<ul>
<li><p>with $A \in R^{m<em>n},rank A=r$, satisfies $U\in R^{m</em>r},U^TU = I,V \in R^{n*r},V^TV=I$</p>
</li>
<li><p>for $\sigma_1 \ge \sigma_2 \ge …\ge \sigma_r &gt;0$</p>
<script type="math/tex; mode=display">
A=\sum_{i=1}^r \sigma_i u_iv_i^T</script></li>
</ul>
<p><strong>norm</strong></p>
<ul>
<li><script type="math/tex; mode=display">
\left<X,Y\right>=\tr(X^TY)</script></li>
<li><p><strong>nuclear norm</strong></p>
</li>
<li><script type="math/tex; mode=display">
||A||_*=\sum_i \sigma_i(A)=\max_{U^TU=I,V^TV=I} \tr(U^TXV)</script></li>
<li><p>usually used for approximating the rank of a matrix.</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://web.stanford.edu/~pilanci/papers/ConvexAttention2022.pdf">ConvexAttention2022.pdf (stanford.edu)</a></p>
</li>
<li><p><strong>(p,q)-norm</strong> usually used in sparse representation.</p>
</li>
</ul>
<p><strong>dual norm</strong></p>
<script type="math/tex; mode=display">
||z||^*=\sup \{z^Tx \mid ||x|| \le 1\}</script><p>性质有</p>
<script type="math/tex; mode=display">
\forall x,\exist x^*,\left<x,x^*\right>=||x||||x^*||^*</script><p>以及知名的Cauchy-Schwartz inequality</p>
<script type="math/tex; mode=display">
z^Tx \le ||x|| ||z||^*</script><p><strong>condition number</strong></p>
<ul>
<li><script type="math/tex; mode=display">
cond(A)=||A||_2 ||A^{-1}||_2=\sigma_{max}(A)/\sigma_{min}(A)</script></li>
<li><p>描述了矩阵对向量的拉伸和压缩能力。</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/91393594">矩阵的条件数 - 知乎 (zhihu.com)</a></p>
</li>
<li><blockquote>
<p>对于 $Ax=b$，条件数唯一决定了线性方程的解关于观测的噪声的影响程度。条件数越大越严重。（x的变化率<strong>偏离</strong>b的变化率就更大）</p>
</blockquote>
</li>
</ul>
<p><strong>Adjoint operator</strong></p>
<ul>
<li><script type="math/tex; mode=display">
\left<A^*(x),y\right> =\left<x,A(y)\right>,\forall x\in \mathbb R^n,y\in \mathbb R^m</script></li>
</ul>
<p><strong>von Neumann trace theorem</strong></p>
<ul>
<li><p>Suppose $A,B \in R^{m*n}$，then</p>
</li>
<li><script type="math/tex; mode=display">
|\left<A,B\right>| \le \sum_{i=1}^{\min(m,n)} \sigma_i(A)\sigma_i(B)</script></li>
</ul>
<p><strong>convex set</strong></p>
<ul>
<li>every two points can see each other.</li>
</ul>
<blockquote>
<p>$\empty, {x_0},R^n$ </p>
</blockquote>
<p><strong>convex hull</strong></p>
<ul>
<li>all the convex combination of points</li>
<li>the smallest convex set contains C</li>
</ul>
<blockquote>
<script type="math/tex; mode=display">
\{e_ie_j^T\}\\
\{uu^T \mid ||u||=1\}\\
\{uv^T \mid ||u||=1,||v||=1\}</script></blockquote>
<p><strong>这个习题必须重新做一遍，补一下回放吧</strong></p>
<p><strong>Cones</strong></p>
<ul>
<li>$x\in C,\theta&gt;0 \implies \theta x\in C$</li>
</ul>
<p><strong>convex cone</strong></p>
<ul>
<li>$x_1,x_2\in C,\theta_1,\theta_2 &gt;0,\theta_1x_1+\theta_2x_2 \in C$</li>
</ul>
<p><strong>conic hull</strong></p>
<ul>
<li>conic combination of points.</li>
</ul>
<p><strong>affine set</strong></p>
<ul>
<li>contains all <strong>lines</strong> passing through any two points. if x0 in C, C-x0 is a subspace.</li>
</ul>
<p><strong>dual cone</strong></p>
<ul>
<li><script type="math/tex; mode=display">
K^*=\{y\mid x^Ty \ge 0\text{ for all x}\}</script></li>
<li><p>is closed and convex.</p>
</li>
<li><p>$K_1\subseteq K_2 \implies K_2^<em>\subseteq K_1^</em>$</p>
</li>
<li><p>$K^{**}$ is the closure of the convex hull of $K$.</p>
</li>
</ul>
<blockquote>
<p>subspace, non-negtive orthant, postive semi-definite cone, norm cone.</p>
</blockquote>
<p><strong>Operations that preserve convexity</strong></p>
<ul>
<li><p>Intersection，如 $S<em>+^n=\bigcap</em>{z\not=0}{X\in S^n\mid z^TXz\ge 0}$ </p>
</li>
<li><p><strong>Affine functions</strong></p>
</li>
<li>Sum and Cartesian product</li>
<li><strong>Perspective functions</strong><ul>
<li>$P(\mathrm z,t)=\mathrm z/t$</li>
</ul>
</li>
<li>Linear-fractional functions</li>
</ul>
<p><strong>Separating hyperplane theorem</strong></p>
<ul>
<li>如果两个凸集是不交的，那么就存在一个向量 $a\not=0,b$ ，使得 $a^Tx-b$ 再一个集合大于等于零，一个集合小于等于零。</li>
<li>如果是严格不等，叫做Strictly separation</li>
<li>如果两个凸集至少有一个是开集，那么只要存在分割平面他们就不交。</li>
</ul>
<p>考虑强二择一定理，即<strong>Faka’s Lemma</strong></p>
<script type="math/tex; mode=display">
1) \exist x\in R_+^n, Ax=b\\
2)\exist y \in R^m, A^Ty \ge 0,b^Ty<0</script><p>第一个如果有 $Ax=b,A^Ty\ge 0\implies b^Ty=x^TA^Ty\ge 0$</p>
<p>第二个如果 $Ax\not =b$，也就是 $b$ 在 $cone(A)$ 这个集合的外面，那么存在一个 $y$，使得 $\left<y,Ax\right> \ge \mu \ge \left<y,b\right>$。然后因为 $A^Ty \ge 0$ 且 $x \ge 0$，所以 $\mu \le 0$。</y,b\right></y,Ax\right></p>
<p><strong>Supporting hyperplanes</strong></p>
<ul>
<li>非空凸集的任何一个边界上的点，至少存在一个支撑超平面。</li>
<li>也就是一个向外的方向 $a$，对于边界上的点 $x_0$，使得 $a^Tx \le a^Tx_0$。</li>
</ul>
<h3 id="Convex-function"><a href="#Convex-function" class="headerlink" title="Convex function"></a>Convex function</h3><ul>
<li>定义线性组合的函数值小于函数值的线性组合。</li>
</ul>
<p><strong>Rademacher’s Theorem</strong> 凸函数的几乎所有相对内点都可微。</p>
<p><strong>Extended-value extensions</strong></p>
<ul>
<li>对于凸函数，把对于非定义域的地方扩充为 $\infty$，值域变成了 $R\cup{\infty}$</li>
</ul>
<p><strong>First-order conditions</strong></p>
<ul>
<li><p>如果函数可微，那么这个条件和函数凸等价</p>
</li>
<li><script type="math/tex; mode=display">
f(y)\ge f(x)+\left<\nabla f(x),y-x\right></script></li>
<li><p>对于所有的 $x,y \in dom f$ 成立。</p>
</li>
<li><p>可以写成</p>
</li>
<li><script type="math/tex; mode=display">
f(y) \ge f(x)+ \frac{f(x+\alpha(y-x))-f(x)}{\alpha}</script></li>
<li><p><strong>Strongly Convex</strong>: $f(y)\ge f(x)+\left&lt;\nabla f(x),y-x\right&gt;+\frac{\mu}{2}||y-x||^2$</p>
</li>
</ul>
<p><strong>Second-order conditions</strong>: $\nabla^2 f(x) \succeq 0$</p>
<ul>
<li>Strongly convex: $\nabla^2 f(x) \succeq \mu I$,</li>
<li>but for strictly convex, $\nabla^2f(x) \succeq 0$ ,and $\succ 0$ <strong>not holds</strong>.</li>
</ul>
<p><strong>sublevel</strong></p>
<ul>
<li><script type="math/tex; mode=display">
C_\alpha =\{x\in domf \mid f(x)\le \alpha\}</script></li>
</ul>
<p><strong>quasi-convex</strong></p>
<ul>
<li>$f(\alpha x+(1-\alpha)y)\le \max [f(x),f(y)],\alpha \in [0,1]$</li>
</ul>
<p><strong>epigraph</strong></p>
<p><strong>proper function</strong></p>
<ul>
<li>$f(x)&lt;\infty$ for at least one x, and $f(x)&gt;-\infty$ for all $x \in \mathcal X$ .</li>
</ul>
<p><strong>Jesen’s inequality</strong></p>
<ul>
<li>$f(\mathbb E x)\le \mathbb E f(x)$</li>
</ul>
<p><strong>Bregman distrance</strong></p>
<ul>
<li>$B_f(y,x)=f(y)-f(x)-\left&lt;\nabla f(x),x\right&gt;$</li>
</ul>
<p><strong>subgradient</strong></p>
<ul>
<li>$\partial f(x)={g \mid f(y) \ge f(x)+\left<g,y-x\right>}$</g,y-x\right></li>
<li>可以加法分解</li>
<li>可以链式法则，这里如果 $F=h(f(x))$在 $h$ 是convex 以及单调非递减的时候可以链式法则分解。</li>
</ul>
<p><strong>Danskin’s Theorem</strong></p>
<ul>
<li>TBD</li>
</ul>
<p><strong>subdifferential of norms</strong></p>
<ul>
<li><script type="math/tex; mode=display">
\partial ||x|| = \{y \mid \left<y,x\right>=||x||,||y||^*\le 1 \}</script></li>
<li><p>证明分两步，一定要满足条件是容易的，还有一种是满足条件的一定是subgradint</p>
</li>
<li><p>用定义，$\left<y,w-x\right>=\left<y,w\right>-||x||\le ||y^*||||w||-||x||\le ||w||-||x||$</y,w\right></y,w-x\right></p>
</li>
</ul>
<p><strong>函数保凸运算</strong></p>
<ul>
<li>Nonnegative weighted sums</li>
<li>Composition with an affine mapping，$g(x)=f(Ax+b)$</li>
<li><p><strong>Pointwise maximum and supremum</strong>，</p>
</li>
<li><p><strong>Composition – Scalar composition</strong></p>
<ul>
<li>$f(x)=h(g(x))$</li>
<li>有两种情况</li>
<li>h是convex而且非递减的，那么g是convex的</li>
<li>h是convex且非递增的，那么g是concave的。</li>
<li>从 $f’’(x)=h’’(g(x))g’(x)^2+h’(g(x))g’’(x)$ 可以想到</li>
</ul>
</li>
<li><p>比如 $-\log (-g(x)),\exp g(x)$ 在 $g(x)$ convex的时候</p>
</li>
<li><p><strong>Composition – Vector composition</strong></p>
<ul>
<li>$h(g(x))=h(g_1(x)…g_k(x))$</li>
<li>条件和上面一样的，不过对每个g都成立。</li>
</ul>
</li>
<li><p><strong>Minimization</strong></p>
<ul>
<li><script type="math/tex; mode=display">
g(x)=\inf_{y \in C} f(x,y)</script></li>
<li><p><strong>epi g是epi f的一个投影！</strong></p>
</li>
</ul>
</li>
<li><p><strong>Perspective of a function</strong></p>
<ul>
<li>$g(x,t)=tf(x/t)$。</li>
<li><strong>epi g</strong> 是 <strong>epi f</strong> 的inverse mapping</li>
<li>比如取 $f(x)=-\log x$，就能得到相对熵是凸的。</li>
</ul>
</li>
</ul>
<p><strong>conjugate function</strong></p>
<script type="math/tex; mode=display">
f^*(y)=\sup_{x} \left<y,x\right>-f(x) =\sup_{(x,t)\in epi_f}\pmatrix{y\\-1}^T\pmatrix{x\\t}</script><p>（强烈建议看那张图）</p>
<p><strong>examples</strong></p>
<ul>
<li>affine function</li>
<li>negative logarithm</li>
<li>exponential</li>
<li>negative entropy: $f(x)=x\log x,[xy-x\log x]f^*(y)=e^{y-1}$</li>
<li>inverse</li>
</ul>
<p><strong>Fenchel’s inequality</strong></p>
<script type="math/tex; mode=display">
f(x)+f^*(x) \ge \left<x,y\right></script><p>such like take $f(x)=\frac{1}{2}||x||^2$ or $f(x)=\frac{1}{2}x^TQx$</p>
<p><strong>conjugate of the conjugate</strong></p>
<ul>
<li>If $f$ is proper, convex and closed, then $f^{**}=f$</li>
</ul>
<script type="math/tex; mode=display">
\begin{align*}
f^{**}(x)&=\sup_y(x,y)-(\sup_x (x,y)-f(x))\\
&= (x,y^*)-(\sup_{x'} (x',y^*)-f(x'))\\
&= (x,y^*)-[(x',y^*)-f(x')]\\
&\le (x,y^*)-[(x,y^*)-f(x)]\\
&=f(x)
\end{align*}</script><ul>
<li>Suppose $\exists (x_0^T,\gamma)^T \not \in epi f$ , where $\gamma \ge f^{**}(x_0)$. Since $f$ is proper, convex, closed, its epigraph is closed and does not include a vertical line. Thus <script type="math/tex; mode=display">
(w^T,\zeta)(x^T,t)^T < \mu<(w^T,\zeta)(x_0^T,\gamma)^T,x\in dom f,t\ge f(x)</script>since t can very large, let $\zeta$ must be negative. assume $\zeta = -1$, take $t=f(x)$<script type="math/tex; mode=display">
w^Tx-f(x)<\mu<w^TTx_0-\gamma\le w^Tx_0-f^{**}(x);\forall x\in domf</script>Thus $f^<em>(w)&lt;w^Tx_0-f^{*</em>}(x)$</li>
</ul>
<p>contradicting Fenchel’s inequality</p>
<ul>
<li><p>For any $f$, $f^{**}$ is the largest convex function not exceeding f.</p>
</li>
<li><p>if $f(u,v)=f_1(u)+f_2(v)$, then $f^<em>(w,z)=f_1^</em>(w)+f_2^*(z)$</p>
</li>
</ul>
<p><strong>Envelope function and Proximal  mapping</strong></p>
<ul>
<li>$Env_c f(x)=\inf_w{f(w)+\frac{1}{2c}||w-x||^2}$</li>
<li>$Prox_c f(x)=\arg\min_w{f(w)+\frac{1}{2c}||w-x||^2}$</li>
</ul>
<script type="math/tex; mode=display">
u=Prox_cf(x)\iff\frac{1}{c}(x-u)\in \partial f(u)</script><script type="math/tex; mode=display">
f(x_{k+1})-f(x_k)\le \frac{1}{-2\alpha}||x_{k+1}-x_k||^2</script><p><strong>proximal gradient</strong></p>
<ul>
<li><script type="math/tex; mode=display">
\min _x f(x)+g(x)</script></li>
<li><p>where $g$ is L-smooth, $f$ non-differentiable but having easily computing proximal mapping</p>
</li>
<li><script type="math/tex; mode=display">
g(x)\le g(x_k)+\left<\nabla g(x_k),x-x_k\right>+\frac{L}{2}||x-x_k||^2</script></li>
<li><script type="math/tex; mode=display">
\min _x f(x)+\frac{L}{2}||x-x_k+L^{-1}\nabla g(x_k)||^2</script></li>
</ul>
<p>if $f(u,v)=f_1(u)+f_2(v)$, both closed proper functions.</p>
<script type="math/tex; mode=display">
Prox_c f(u,v)=(Prox_cf_1(u),Prox_cf_2(v))</script><p>能证明</p>
<script type="math/tex; mode=display">
\nabla Env_c f(x)=\frac{1}{c}(x-Prox_c f(x))</script><p>也就是</p>
<script type="math/tex; mode=display">
\frac{1}{c}(x-u)\in \partial Env_c f(x)</script><p><strong>Moreau Decomposition</strong></p>
<script type="math/tex; mode=display">
x=Prox_c f(x)+c Prox_{c^{-1}} f^*(c^{-1}x)</script><p>proximal mapping</p>
<p>if f is convex, the proximal operator is 1-Lipschitz and the envelope function is (1/c)-smooth.</p>
<p><strong>proximal mapping of a norm</strong></p>
<script type="math/tex; mode=display">
f(x)=||x||,f^*(y)=I_{B}(y)\\
Prox_c f(x)=x-cProx_{c^{-1}}f^*(x/c)\\
=x-cP_{cB}(x/c)\\
==x-P_{cB}(x)</script><p>where B is the unit ball of the dual norm $||\cdot ||_*$</p>
<h3 id="Unconstrained-Optimization"><a href="#Unconstrained-Optimization" class="headerlink" title="Unconstrained Optimization"></a>Unconstrained Optimization</h3><p><strong>Descent methods</strong></p>
<script type="math/tex; mode=display">
\nabla f(x^{(k)})^T(y-x^{(k)})\ge 0</script><p>停止准则是 $||\nabla f(x)||_2\le \eta$</p>
<p> Exact line search: 去求min</p>
<p>Backtracking line search: while $f(x+t\Delta x)&gt;f(x)+\alpha t \nabla f(x)^T\Delta x$, do $t=\beta t$</p>
<p><strong>gradient descent method</strong>: $\Delta x=-\nabla f(x)$</p>
<p>如果强凸则有</p>
<script type="math/tex; mode=display">
\nabla ^2 f(x)\succeq mI\\
f(y)\ge f(x)+\nabla f(x)^T(y-x)+\frac{m}{2}||y-x||_2^2</script><p>在这种情况下，一旦 $\nabla f(x)$ 足够小，我们就有对于任何一个 $y$，取 $\tilde y=x-(1/m)\nabla f(x)$</p>
<script type="math/tex; mode=display">
p^*\ge f(y)\ge f(x)+\nabla f(x)^T(y-x)+\frac{m}{2}||y-x||_2^2\\
\ge f(x)+\nabla f(x)^T(\tilde y-x)+\frac{m}{2}||y-x||_2^2\\
= f(x)-\frac{1}{2m}||\nabla f(x)||_2^2</script><p>上面的结论是</p>
<script type="math/tex; mode=display">
||\nabla f(x)||_2^2 \ge 2m(f(x)-p^*)</script><p>另外我们还有一个upperbound，然后有一个 <strong>descent lemma</strong></p>
<p>如果M-smooth，最大特征值被bound</p>
<script type="math/tex; mode=display">
MI\succeq \nabla^2f(x)</script><p>那么</p>
<script type="math/tex; mode=display">
f(y)\le f(x)+\nabla f(x)^T(y-x)+\frac{M}{2}||y-x||_2^2</script><p>也就有</p>
<script type="math/tex; mode=display">
p^*\le f(x)-\frac{1}{2M}||\nabla f(x)||_2^2</script><p>也就有对于新的 $x^+$，对于exact line search</p>
<script type="math/tex; mode=display">
f(x^+)-p^*\le f(x)-\frac{1}{2M}||\nabla f(x)||_2^2\le (1-m/M)(f(x)-p^*)</script><p>linear convergence.</p>
<p>对于backtracking line search，要分情况讨论，关于有没有发生，这里看不懂跳过。</p>
<p><strong>steepest descent method</strong></p>
<ul>
<li>对于二模的最速方向确实就是 $-\nabla f(x)$，但是对于矩阵 $P$ 下的模是，$-P^{-1}\nabla f(x)$，需要归一化一下。</li>
</ul>
<p>对于L1的则是：</p>
<script type="math/tex; mode=display">
\arg\min_{v} \set{\nabla f(x)^Tv\mid ||v||_1\le 1}</script><p>算出来是最大的方向的负。</p>
<p><strong>newton’s method</strong></p>
<script type="math/tex; mode=display">
0=g^{(k)}+F(x^{(k)})(x-x^{(k)})\\
x_{k+1}=x_k-F^{-1}g</script><p>Theorem 2: Newton’s Method for Strongly Convex Functions</p>
<p><strong>Theorem:</strong> Suppose that $f \in C^2$ is strongly convex, with Lipschitz continuous second order derivative:</p>
<script type="math/tex; mode=display">
\nabla^2 f(x) \ge mI, \quad ||\nabla^2 f(x) - \nabla^2 f(y)||_F \le L||x - y||^2,</script><p>and $x^<em> \in \mathbb{R}^n$ is a local minimizer. Then, for all $x^{(0)}$ sufficiently close to $x^</em>$, Newton’s method converges to $x^*$ with order of convergence at least 2.</p>
<blockquote>
<p><strong>Proof:</strong></p>
<p>Let $x^{(k)}$ be the $k$th iterate of Newton’s method, defined by</p>
<script type="math/tex; mode=display">
x^{(k+1)} = x^{(k)} - \nabla^2 f(x^{(k)})^{-1} \nabla f(x^{(k)}).</script><p>Then, we can write</p>
<script type="math/tex; mode=display">
f(x^{(k+1)}) = f(x^{(k)}) + \nabla f(x^{(k)})^T (x^{(k+1)} - x^{(k)}) + \frac{1}{2} (x^{(k+1)} - x^{(k)})^T \nabla^2 f(x^{(k)}) (x^{(k+1)} - x^{(k)}).</script><p>Using the strong convexity of $f$, we have</p>
<script type="math/tex; mode=display">
f(x^{(k+1)}) \ge f(x^{(k)}) + \nabla f(x^{(k)})^T (x^{(k+1)} - x^{(k)}) + \frac{m}{2} ||x^{(k+1)} - x^{(k)}||^2.</script><p>Expanding the first term on the right-hand side, we get</p>
<script type="math/tex; mode=display">
\nabla f(x^{(k)})^T (x^{(k+1)} - x^{(k)}) = - \frac{1}{2} (x^{(k+1)} - x^{(k)})^T \nabla^2 f(x^{(k)}) (x^{(k+1)} - x^{(k)}).</script><p>Substituting this into the inequality above, we get</p>
<script type="math/tex; mode=display">
f(x^{(k+1)}) \ge f(x^{(k)}) - \frac{1}{2} (x^{(k+1)} - x^{(k)})^T \nabla^2 f(x^{(k)}) (x^{(k+1)} - x^{(k)}) + \frac{m}{2} ||x^{(k+1)} - x^{(k)}||^2.</script><p>Completing the square on the right-hand side, we get</p>
<script type="math/tex; mode=display">
f(x^{(k+1)}) \ge f(x^{(k)}) + \frac{1}{2} ||x^{(k+1)} - x^{(k)} - \nabla^2 f(x^{(k)})^{-1} \nabla f(x^{(k)})||^2 \left( m - \frac{L}{m} \right).</script><p>Since $m &gt; 0$ and $L &gt; 0$, we can choose $\delta &gt; 0$ such that $m - \frac{L}{m} \ge \delta$ for all $x^{(k)}$ sufficiently close to $x^*$. Then, we have</p>
<script type="math/tex; mode=display">
f(x^{(k+1)}) \ge f(x^{(k)}) + \frac{\delta}{2} ||x^{(k+1)} - x^*||^2.</script><p>This shows that Newton’s method converges to $x^*$ with quadratic rate of convergence.</p>
</blockquote>
<p><strong>conjugate gradient algorithm</strong></p>
<script type="math/tex; mode=display">
f(x)=\frac{1}{2}x^TQx-x^Tb\\
d^{(0)}=-g^{(0)}\\
\alpha_k=\arg \min_{\alpha>0} f(x^{(k)}+\alpha d^{(k)})=\frac{g^{(0)T}d^{(0)}}{d^{(0)T}Qd^{(0)}}\\
g^{(k+1)}=\nabla f(x^{(k+1)})\\
\beta_k=\frac{g^{(k+1)T}Qd^{(k)}}{d^{(k)T}Qd^{(k)}}\\
d^{(k+1)}=-g^{(k+1)}+\beta_k d^{(k)}</script><p>目的是包括</p>
<script type="math/tex; mode=display">
\lang g^{(k+1)T},d^{(i)}\rang=0\\
\lang g^{(k+1)T},g^{(i)}\rang=0</script><p>此外所有的 $Q^{1/2} d^{(i)}$ 也是两两正交的，也就是要 $d^{(i)}Qd^{(j)}=0$</p>
<p><strong>Quasi-Newton Methods</strong></p>
<ul>
<li>估计hess矩阵</li>
</ul>
<script type="math/tex; mode=display">
g^{(k+1)}-g^{(k)}=Q(x^{(k+1)}-x^{(k)})</script><p><strong>Rank One Correction Formula</strong></p>
<script type="math/tex; mode=display">
H_{k+1}=H_k+ a_kz^{(k)}z^{(k)T}</script><p>要让</p>
<script type="math/tex; mode=display">
H_{k+1}\Delta g^{(k)}=\Delta x^{(k)}</script><p>展开解这个 $z^{(k)}$</p>
<ul>
<li>但是rank one的修正有问题，不保证positive definition</li>
</ul>
<p><strong>DFP algorithm</strong></p>
<ul>
<li>也是一个公式，但真消化不了这么多了。</li>
<li><strong>先pass吧，考了也没办法</strong>。</li>
</ul>
<p><strong>BFGS algorithm</strong></p>
<ul>
<li>维护一个矩阵 $B$，$B$ 是用DFP的方法来更新的。</li>
<li>然后用 $B_{k+1}^{-1}$ 来估计 BFGS的H，然后求逆可以直接展开。</li>
</ul>
<p><strong>L-BFGS</strong></p>
<ul>
<li>只用几个变量来估计 $B$</li>
</ul>
<p><strong>Majorization Minimization</strong></p>
<ul>
<li>就那个MM算法，看之前的博客吧，但也学不会。</li>
</ul>
<h4 id="optimality-conditions"><a href="#optimality-conditions" class="headerlink" title="optimality conditions"></a>optimality conditions</h4><p>local first order condition</p>
<p><strong>KKT condition</strong></p>
<ol>
<li><script type="math/tex; mode=display">
\nabla f(x_0)+\sum_{i=1}^m \lambda_i \nabla g_i(x_0)+\sum_{j=1}^p \mu_j \nabla h_j(x_0)=0</script></li>
<li><script type="math/tex; mode=display">
\lambda_j g_i(x_0)=0</script></li>
<li><script type="math/tex; mode=display">
x_0 \in F</script></li>
<li><script type="math/tex; mode=display">
\lambda \in R_{+}^m</script></li>
</ol>
<p>KKT-point $(x,\lambda,\mu)$</p>
<p><strong>SCQ condition</strong></p>
<p>The functions $g_i$ are convex for all $i\in \mathcal I$</p>
<script type="math/tex; mode=display">
\exist x\in F,g_i(x)<0,\forall  i \in \mathcal I_{1}</script><p>where $\mathcal I_{1}$ is the index set of nonlinear constraints.</p>
<p>=&gt;<strong>ACQ</strong> $c_l(x_0)=c_t(x_0)$ =&gt;<strong>GCQ</strong>: $C_l(x_0)^<em>=C_t(x_0)^</em>$.</p>
<p><img src="https://cdn.luogu.com.cn/upload/image_hosting/itmzw9y5.png" alt="img"></p>
<h4 id="duality"><a href="#duality" class="headerlink" title="duality"></a>duality</h4><ul>
<li>如果SCQ就是strong duality的。</li>
</ul>
<p>不是我真学不会啊啊啊啊</p>
<h3 id="Unconstrained-Optimization-1"><a href="#Unconstrained-Optimization-1" class="headerlink" title="Unconstrained Optimization"></a>Unconstrained Optimization</h3><p><strong>projected gradient descent</strong></p>
<p><strong>dual method</strong></p>
<p><strong>Newton’s method with equality constraints</strong></p>
<p><strong>penalty method</strong></p>
<p><strong>frank-wolfe algorithm</strong></p>
<ul>
<li>直接在定义域内做线性化，找到线性化之后定义域内最优的那个点，$s_k$，让$s_k-x_k$作为下降方向。</li>
</ul>
<p><strong>Lagrangian method of multiplier</strong></p>
<p><strong>augmented Lagrangians and the method of multiplier</strong></p>
<script type="math/tex; mode=display">
L_\beta(x,\lambda)=f(x)+\lambda^T(Ax-b)+(\beta/2)||Ax-b||_2^2</script><p><strong>Alternating Direction Method</strong></p>
<ul>
<li><p>$\min_{x,y} f(x)+g(y),s.t. A(x)+B(y)=c$, where f and g are convex, A and B are linear mappings.</p>
</li>
<li><p>the augmented Lagrangian function:</p>
</li>
<li><script type="math/tex; mode=display">
L(x,y,\lambda)=f(x)+g(y)+\left<\lambda,A(x)+B(y)-c\right>+\beta/2||A(x)+B(y)-c||^2</script></li>
</ul>
<p>then </p>
<script type="math/tex; mode=display">
\begin{align*}
x_{k+1}&=\arg \min_x L(x,y_k,\lambda_k)\\
&=\arg \min_x f(x)+\frac{\beta}{2}||A(x)+B(y_k)-c+\lambda_k/\beta||^2\\
y_{k+1}&=\arg \min_x L(x_{k+1},y,\lambda_k)\\\\
&=\arg \min_{y} g(y)+\frac{\beta}{2}||B(y)+A(x_{k+1})-c+\lambda_k/\beta||^2
\\\lambda_{k+1}&=\lambda_k+\beta (A(x_{k+1})+B(y_{k+1})-c)
\end{align*}</script><ul>
<li>$\beta$ can be $\tau \beta$, $\tau\in(0,(1+\sqrt 5)/2)$</li>
<li>我们假定那个argmin是容易的，才容易把这个做下去。</li>
</ul>
<p><strong>example: RPCA(robost PCA)</strong></p>
<p><strong>LADM</strong></p>
<ul>
<li>把二次项做线性化，</li>
</ul>
<p>这里面最有意思的是，两种闭解的形式。</p>
<p>一种是一范数的proximal的close form，类似于</p>
<script type="math/tex; mode=display">
y_{k+1}=S_{\beta^{-1}} (x_{k+1}+\lambda_k/\beta-b)+b\\S_{\epsilon}=\text{sgn}(x) \max(|x|-\epsilon,0)</script><p>一种是二范数的proximal的close form，类似于</p>
<script type="math/tex; mode=display">
x_{k+1}=\frac{\beta (-a+y_k-\lambda_k/\beta)}{\beta+1}+a</script><p><strong>辅助变量等价变形</strong></p>
<ul>
<li>原来的问题如何做一个re-fomulation让它更好解。</li>
<li>如果一个东西是有close-form solution的话就比较好解。</li>
</ul>
<p><strong>LADMAP</strong></p>
<blockquote>
<p> Lin et al., Linearized Alternating Direction Method with Adaptive Penalty for Low-Rank Representation, NIPS 2011.</p>
</blockquote>
<script type="math/tex; mode=display">
\beta_{k+1}=\min(\beta_{max},\rho \beta_k)</script><p><strong>LADMPSAP</strong></p>
<ul>
<li>对于三个以上的，同时更新。</li>
</ul>
<p><strong>Proximal LADMPSAP</strong></p>
<ul>
<li></li>
</ul>
<p><strong>concensus problem</strong></p>
<script type="math/tex; mode=display">
\min_x \sum_{i=1}^n f_i(x)</script><p>这个问题叫做 composite</p>
<script type="math/tex; mode=display">
\min_{x_1,...x_n} \sum_{i=1}^n f_i(x_i)</script><p>这个问题叫 separable</p>
<p><strong>Group Sparse Logistic Regression with Overlap</strong></p>
<script type="math/tex; mode=display">
\min_{w,b}\frac{1}{s}\sum_{i=1}^S \log(1+\exp (-y_i(w^Tx_i+b))) + \mu\sum_{j=1}^t||S_jw||</script><p>where $S_j$ are the selection matrices.</p>
<p>rewrite $z=\bar S\bar w,\bar S=(S,0),S=(S_1^T,…S_t^T)^T$, and $\bar w=(w^T,b)^T$.</p>
<p>关于 $h(i)$ 的 proximal 算子，另一个是 soft-thresholding operation.</p>
<blockquote>
<p>没有要求大家掌握这个证明，只要会结论的参数的证明，</p>
<p>线性化和proximal还是要好好掌握的。</p>
</blockquote>
<hr>
<p><strong>Coordinate Descent</strong></p>
<ul>
<li>一个拍脑袋也能想到的算法</li>
<li>$x<em>{i}^{k+1}=\arg \min</em>{x<em>i} f(x_1^{k+1},x_2^{k+1},…x</em>{i-1}^{k+1},x<em>i,x</em>{i+1}^k,…x_n^k)$</li>
<li>基本原则就是每个subproblem都要好解。</li>
</ul>
<p><strong>Block Coordinate Descent</strong></p>
<ul>
<li>Convergence: 收敛性有个很直观的反例。</li>
</ul>
<p><strong>example: adaboost</strong></p>
<p><strong>example: Sequential Minimal Optimization</strong></p>
<script type="math/tex; mode=display">
\begin{align*}
\max_{\boldsymbol{\lambda}} &\quad \sum_{i=1}^m \lambda_i - \frac{1}{2}\sum_{i=1}^m \sum_{j=1}^m \lambda_i \lambda_j y_i y_j \left( x_i \cdot x_j\right)  \\
\text{s.t.} &\quad \sum_{i=1}^m \lambda_i y_i = 0 \\
&\quad \lambda_i \geq 0, \quad i = 1,\ldots,m.
\end{align*}</script><p>选择两个变量 $\lambda<em>1,\lambda_2$ 违反了KKT条件，让 $\lambda_i,i\not=1,2$ 固定，且 $\lambda_1y_1+\lambda_2y_2=C=-\sum</em>{i\not=1,2}\lambda_iy_i$，用 $\lambda_2=y_2(C-\lambda_1y_1)$ 替代，变成（$\alpha,\beta$ 是一些常数）</p>
<script type="math/tex; mode=display">
\min_{\lambda_1} \alpha \lambda_1^2+\beta \lambda_1,\\
\text{s.t.}\quad \lambda_1\ge 0,y_2(C-\lambda_1y_1)\ge 0</script><p>这东西有一个close form的solution。</p>
<h4 id="Randomized-Algorithms"><a href="#Randomized-Algorithms" class="headerlink" title="Randomized Algorithms"></a>Randomized Algorithms</h4><p><strong>Stochastic Gradient Descent (SGD)</strong></p>
<p><strong>Theorem1</strong></p>
<script type="math/tex; mode=display">
w^*\in \arg \min_{w:||w||\le B} f(w)</script><p>assume SGD is run for $T$ times, with $\eta=\sqrt {\frac{B^2}{\rho^2T}}$. Assume also that for all $t$, $v_t \le \rho$ with probability 1, then</p>
<script type="math/tex; mode=display">
E[f(\bar w)]-f(w^*)\le \frac{B\rho}{\sqrt T}</script><p>注意是平均意义上的收敛。</p>
<p>这个的证明分成两步，第一个是</p>
<p>如果 $w^{(t+1)}=w^{(t)}-\eta v_t$，</p>
<script type="math/tex; mode=display">
\begin{align*}
\langle \mathbf{w}^{(t)} - \mathbf{w}^*, \mathbf{v}_t \rangle 
&= \frac{1}{\eta} \langle \mathbf{w}^{(t)} - \mathbf{w}^*, \eta \mathbf{v}_t \rangle \\
&= \frac{1}{2\eta} \left( - \|\mathbf{w}^{(t)} - \mathbf{w}^* - \eta \mathbf{v}_t\|^2 + \|\mathbf{w}^{(t)} - \mathbf{w}^*\|^2 + \eta^2 \|\mathbf{v}_t\|^2 \right) \\
&= \frac{1}{2\eta} \left( - \|\mathbf{w}^{(t+1)} - \mathbf{w}^*\|^2 + \|\mathbf{w}^{(t)} - \mathbf{w}^*\|^2 \right) + \frac{\eta}{2} \|\mathbf{v}_t\|^2.
\end{align*}</script><p>满足</p>
<script type="math/tex; mode=display">
\sum_{t=1}^T\left<w^{(t)}-w^*,v_t\right>\le \frac{||w^*||^2}{2\eta}+\frac{\eta}{2} \sum_{t=1}^T ||v_t||^2</script><p>注意这里如果让 $\eta=\frac{B}{\rho\sqrt T}$ 就有</p>
<script type="math/tex; mode=display">
\frac{1}{T}\sum_{t=1}^T\left<w^{(t)}-w^*,v_t\right> \le \frac{B\rho}{\sqrt T}</script><p>第二个是：</p>
<script type="math/tex; mode=display">
E_{v_{1:T}}\left[\frac{1}{T}\sum_{t=1}^T\left<w^{(t)}-w^*,v_t\right>\right]</script><p>对于每一个有</p>
<script type="math/tex; mode=display">
\begin{align*}
\mathbb{E}_{\mathbf{v}_{1:T}} \left[ \langle \mathbf{w}^{(t)} - \mathbf{w}^*, \mathbf{v}_t \rangle \right]
=& \mathbb{E}_{\mathbf{v}_{1:t}} \left[ \langle \mathbf{w}^{(t)} - \mathbf{w}^*, \mathbf{v}_t \rangle \right] 
\\=& \mathbb{E}_{\mathbf{v}_{1:t-1}} \mathbb{E}_{\mathbf{v}_{1:t}} \left[ \langle \mathbf{w}^{(t)} - \mathbf{w}^*, \mathbf{v}_t \rangle \big| \mathbf{v}_{1:t-1} \right] 
\\=& \mathbb{E}_{\mathbf{v}_{1:t-1}} \left\langle \mathbf{w}^{(t)} - \mathbf{w}^*, \mathbb{E}_{\mathbf{v}_t} \left[ \mathbf{v}_t \big| \mathbf{v}_{1:t-1} \right] \right\rangle 
\\\geq& \mathbb{E}_{\mathbf{v}_{1:t-1}} \left[ f(\mathbf{w}^{(t)}) - f(\mathbf{w}^*) \right] 
\\=& \mathbb{E}_{\mathbf{v}_{1:T}} \left[ f(\mathbf{w}^{(t)}) - f(\mathbf{w}^*) \right].
\end{align*}</script><p><strong>variant1</strong></p>
<p>如果加一步，每次迭代之后做一个argmin，</p>
<p>因为 $||w-u||^2\ge ||v-u||^2$，如果 $v=\arg\min_{x\in \mathcal H} ||x-w||^2$，所以</p>
<script type="math/tex; mode=display">
-||w^{(t+\frac{1}{2})}-w^{}||^{2}+||w^{(t)}-w^{}||^{2}\le-||w^{(t+1)}-w^{}||^{2}+||w^{(t)}-w^{}||^{2}</script><p>前面的结论依然可以继承下来。</p>
<p>variant2：$\eta_t=\frac{B}{\rho \sqrt t}$</p>
<p>variant3: 可能可以只用partial的平均，而不是所有的平均，只用后 $\alpha T$ 论。</p>
<p>关于 $\lambda$ 强凸：</p>
<script type="math/tex; mode=display">
\eta_t=1/(\lambda t)</script><p>就可以让收敛率提升</p>
<script type="math/tex; mode=display">
\mathbb{E}[f(\hat{w})]-f(w^{*})\le\frac{\rho^{2}}{2\lambda T}(1+log(T)).</script><p>推导是，因为</p>
<script type="math/tex; mode=display">
\left<w(t) - w^*, \nabla (t)\right> \geq f(w^{(t)}) - f(w^*) + \frac{\lambda}{2}||w(t) - w^*||^2</script><p>代入之前的公式，就多了一项。</p>
<script type="math/tex; mode=display">
\begin{align*}
\sum_{t=1}^{T} \mathbb{E}[f(w^{(t)})] - f(w^{*}) &\leq \mathbb{E}\left[\frac{\|w^{(t)} - w^{*}\|^2 - \|w^{(t+1)} - w^{*}\|^2}{2\eta_t} - \frac{\lambda}{2}\|w^{(t)} - w^{*}\|^2\right] \\
&\quad + \frac{\rho^2}{2\lambda }\sum_{t=1}^{T}\eta_t \leq \frac{\rho^2}{2\lambda}\sum_{t=1}^{T}\frac{1}{t} \leq \frac{\rho^2}{2\lambda }\left(1 + \log\left(T\right)\right).
\end{align*}</script><p>然后这里 $\eta_t$ 的取值就可以几乎完全是通过差分来实现的，就是考虑前后两项在某个地方要消掉。</p>
<p>其它方法：</p>
<ul>
<li>除以梯度平方累计的根号（但不是无量纲化的）</li>
<li><strong>sgn SGD</strong> 要么更新正一要么更新负一</li>
</ul>
<p><strong>Random Coordinate Descent</strong></p>
<ul>
<li><p>选 $\eta=R/L\sqrt{2/nt}$</p>
</li>
<li><p>随机选一个，然后优化。</p>
</li>
<li><blockquote>
<p>lzc：我个人感觉优化算法有一个计算量守恒的定理，但是没法定量描述</p>
</blockquote>
</li>
</ul>
<p><strong>Stochastic ADAM/ADAN</strong></p>
<p>技巧1</p>
<script type="math/tex; mode=display">
\theta_{t+1}=\theta_t-\frac{\eta}{\sqrt{E[g^2]+\epsilon}}\cdot g</script><p>技巧2：两种加速方法，一种是基于momentum的（Adam），一种是Nesterov的</p>
<script type="math/tex; mode=display">
g_k=\nabla f(\theta_k)\\
m_k=(1-\beta_1)m_{k-1}+g_k\\
\theta_{k+1}=\theta_k-\eta m_k</script><p>以及</p>
<script type="math/tex; mode=display">
g_k=\nabla f(\theta_k-\eta(1-\beta_1)m_{k-1})\\
m_k=(1-\beta_1)m_{k-1}+g_k\\
\theta_{k+1}=\theta_k-\eta m_k</script><p>有一个新的formulation</p>
<script type="math/tex; mode=display">
g_k\approx \nabla f(\theta_k)+(1-\beta)(\nabla f(\theta_k)-\nabla f(\theta_{k-1}))</script><p>用这个方法加上动量的一阶矩和二阶矩</p>
<blockquote>
<p>林教授：投了两次没中，觉得应该学一学Adam，虽然也没发表但是已经十几万引用了</p>
</blockquote>
<hr>
<p>什么是凸集</p>
<script type="math/tex; mode=display">
\alpha x_0+(1-\alpha )x_1\in S</script><p>什么是强凸函数</p>
<script type="math/tex; mode=display">
f(y)\ge f(x)+\nabla f(x)^T(y-x)+\frac{\lambda}{2}||y-x||^2</script><p>什么是对偶范数</p>
<script type="math/tex; mode=display">
||y||_*=\arg \min_{||x||\le 1} \lang x,y\rang</script><p>什么是 $L$-smooth</p>
<script type="math/tex; mode=display">
||\nabla f(x)-\nabla f(y)||_2\le L||x-y||_2</script><p>怎么判断是否是凸函数</p>
<p>怎么求对偶问题</p>
<script type="math/tex; mode=display">
L(x,\lambda)=||Ax-b||_2^2+\lambda(||x||_2^2-1);\lambda \ge 0\\
g(\lambda)=\sup_{x} L(x,\lambda);\lambda \ge 0</script><script type="math/tex; mode=display">
2A^T(Ax-b)+2\lambda x=0</script><p>怎么判断是否满足SCQ条件</p>
<ul>
<li>只要看是否存在一个点满足所有非凸的条件。</li>
</ul>
<p>如何给出KKT条件</p>
<ul>
<li>四个要求，缺一不可。</li>
</ul>
<p>矩阵的范数是怎么样的</p>
<ul>
<li>$||A||_{2,1}$ 表示所有行的 $L_2$ 范数的和。</li>
<li>Frobenius范数是所有元素平方的和的根号</li>
<li>谱范数是最大的奇异值</li>
<li>1范数是每一列绝对值之和的最大值</li>
<li>oo范数是每一行的绝对值之和的最大值。记忆方法可以考虑1是竖着的oo是横着的。</li>
</ul>
<p>如何给出proximal</p>
<ul>
<li>$prox<em>cf(V)=\arg \min_A ||A||</em>{2,1}+\frac{1}{2c}||A-V||_F^2$</li>
<li>这个应该是能求出来的闭式解</li>
</ul>
<p>如何计算收敛率</p>
<ul>
<li>如果是 r-linear，就是误差的比率（牛顿法某些情况下）</li>
<li>如果是 Q-linear，就是误差的一个上界的比率。</li>
</ul>
<p>什么是Frank-Wolfe算法</p>
<ul>
<li>定义域内找最小的方向</li>
</ul>
<p>什么是LADM</p>
<ul>
<li>每次优化一个变量，以及一个 $\lambda$，用增强拉格朗日乘子</li>
</ul>
<p>怎么计算伴随算子</p>
<ul>
<li>高等代数的内容</li>
</ul>

      
    </div>
    <div class="article-footer">
      <blockquote class="mt-2x">
  <ul class="post-copyright list-unstyled">
    
    <li class="post-copyright-link hidden-xs">
      <strong>本文链接：</strong>
      <a href="http://emoairx.github.io/blog/2024/05/26/convex%20A&OM-saveme/" title="凸分析与优化方法笔记" target="_blank" rel="external">http://emoairx.github.io/blog/2024/05/26/convex A&OM-saveme/</a>
    </li>
    
    <li class="post-copyright-license">
      <strong>版权声明： </strong> 本博客所有文章除特别声明外，均采用 <a href="http://creativecommons.org/licenses/by/4.0/deed.zh" target="_blank" rel="external">CC BY 4.0 CN协议</a> 许可协议。转载请注明出处！
    </li>
  </ul>
</blockquote>


<div class="panel panel-default panel-badger">
  <div class="panel-body">
    <figure class="media">
      <div class="media-left">
        <a href="" target="_blank" class="img-burn thumb-sm visible-lg">
          <img src="/blog/images/avatar.jpg" class="img-rounded w-full" alt="">
        </a>
      </div>
      <div class="media-body">
        <h3 class="media-heading"><a href="" target="_blank"><span class="text-dark">emoairx</span><small class="ml-1x">PKU,EECS</small></a></h3>
        <div>春天来了，冬天还会远吗~</div>
      </div>
    </figure>
  </div>
</div>


    </div>
  </article>
  
    
  <section id="comments">
  	
      <div id="vcomments"></div>
    
  </section>


  
</div>

  <nav class="bar bar-footer clearfix" data-stick-bottom>
  <div class="bar-inner">
  
  <ul class="pager pull-left">
    
    <li class="prev">
      <a href="/blog/2024/06/02/InverseRL/" title="Inverse Reinforcement Learning"><i class="icon icon-angle-left" aria-hidden="true"></i><span>&nbsp;&nbsp;Newer</span></a>
    </li>
    
    
    <li class="next">
      <a href="/blog/2024/05/23/deepspeed/" title="(no title)"><span>Older&nbsp;&nbsp;</span><i class="icon icon-angle-right" aria-hidden="true"></i></a>
    </li>
    
    
  </ul>
  
  
  
  <div class="bar-right">
    
  </div>
  </div>
</nav>
  


</main>

  <footer class="footer" itemscope itemtype="http://schema.org/WPFooter">
	
	
    <ul class="social-links">
    	
        <li><a href="https://github.com/emoairx" target="_blank" title="Github" data-toggle=tooltip data-placement=top><i class="icon icon-github"></i></a></li>
        
    </ul>

    <div class="copyright">
    	
        <div class="publishby">
        	Theme by <a href="https://github.com/cofess" target="_blank"> cofess </a>base on <a href="https://github.com/cofess/hexo-theme-pure" target="_blank">pure</a>.
        </div>
    </div>
</footer>
  <script src="//cdn.jsdelivr.net/npm/jquery@1.12.4/dist/jquery.min.js"></script>
<script>
window.jQuery || document.write('<script src="js/jquery.min.js"><\/script>')
</script>

<script src="/blog/js/plugin.min.js"></script>


<script src="/blog/js/application.js"></script>


    <script>
(function (window) {
    var INSIGHT_CONFIG = {
        TRANSLATION: {
            POSTS: 'Posts',
            PAGES: 'Pages',
            CATEGORIES: 'Categories',
            TAGS: 'Tags',
            UNTITLED: '(Untitled)',
        },
        ROOT_URL: '/blog/',
        CONTENT_URL: '/blog/content.json',
    };
    window.INSIGHT_CONFIG = INSIGHT_CONFIG;
})(window);
</script>

<script src="/blog/js/insight.js"></script>






   




   
    
  <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/valine"></script>
  <script type="text/javascript">
  var GUEST = ['nick', 'mail', 'link'];
  var meta = 'nick,mail,link';
  meta = meta.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#vcomments',
    verify: false,
    notify: false,
    appId: 'AONthdWWfk53xtyouAX8NhIf-gzGzoHsz',
    appKey: '7lxwWP9F0g0a68t0qEPsTFR9',
    placeholder: 'Just go go',
    avatar: 'mm',
    meta: meta,
    pageSize: '10' || 10,
    visitor: true
  });
  </script>

     







<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>